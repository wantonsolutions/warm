\section{discussion}

In this sections we discuss the limitations, generality, and scalability of our approach.



\textbf{RDMA Failures:}
%%
In our approach the switch updates it's memory prior to the RDMA packet
landing in remote memory. This operation is safe under the assumption that no
packets are reordered after egress from the switch and that all operations
are successful. If a c\&s packet updates switch memory, and then is rejected
by the NIC or endhost a reconciliation of memory must take place. The
signifier to the switch that a failure has occurred is an RDMA c\&s NACK. When
this occurs the switch can dump all of it's soft state and reset. This will
cause the clover protocol to revert to it's default chain walk to learn new
values. Our approach requires only a single successful c\&s operation per key
to rebuild its cache.

\textbf{Read acceleration:}
%%
Our current implementation concentrates of fixing write contention,
however there is no limitation which prevents us from gaining a performance
boost on reads. In future work the same RDMA cache can be used to steer
reads which are issued by clients with stale information.


\textbf{Scalability Implications:}
%%
The advantage of using a TOR is that all operations within a rack can be
serialized. However in many cases this degree of total ordering is not
required. For instance access to a single memory server can be serialized by
performing ordering on a SmartNIC connected to the endhost. Our techniques
could be built into smartnics which would allow for them to scale arbitrarily
under the assumption that writes do not span multiple remote memory machines.


\textbf{Alternative Datastructures}
%%
As noted in~\ref{sec:future} the crux of applying our approach to other
structures is the complexity of the data structures invariants. For Clover
the invariant is simple, all writes must append to the end of the list. To
enforce this invariant the last element of the list must be cached to ensure
that the tail location is known.

The more complicated the structural invariants are to maintain, the greater
the information which must be cached; for example an \textit{ordered} list.
To illustrate the additional complexity of maintaining order consider how
clients could perform inserts. First, like clover, clients could write their
entry to a private memory region. Second two pointers must be written, one
which points to the next item, and another from the prior item to the newly
written one. The client could issue the writes itself, however when the
insert occurs it would need to traverse part of the list to ensure that the
result had been inserted to the correct location and collect a lock on both
the prior and successor items. Enforcing the ordering invariant requires that
the switch cache the entire list.

Ordering is more complex in terms of space to maintain compared to only
appending to a lists tail. The complexity of generalizing our technique to
any data structure being is that the switch must cache all necessary metadata
to maintain a data structures invariants.

We've considered exploring the class of data structures which have either
weak structural invariants, or those which only cost $O(1)$ to check. Additionally some
data structures amortize the cost of operations which require complex
invariants. For instance, rather than storing an ordered list, using a
partially ordered list with fast accesses which can be periodically
transformed with expensive operations to be consistent.

%We are exploring the potential set of future data structures currently. One
%example of a data structure with more complex invariants is a B-Tree. In this
%case ordering must be persevered at each level of the tree, and also some
%operations require that many locks up the tree be obtained. We speculate that
%algorithms used in Clover such as writing to a local scratch space and then
%atomically updating a shared vairable could be used in more complicated
%scenarios as well, such as this.

%\textbf{zipfan 0.75} 
%%
%We chose this because it shows scaling isues before we
%start to hit the hardware issues. There is no good answer to this question.

%\textbf{Why does the switch have to store the last key written per client}
%%
%The last write is not stored. Client writes occur in two parts, a private
%write to their own scratch space, and a commiting atomic c\&s. The write
%which is stored is the outstanding writes, i.e writes which have been placed
%in the local storage, but not yet connected via a commiting operation.










