@inproceedings {the-machine,
author = {Paolo Faraboschi and Kimberly Keeton and Tim Marsland and Dejan Milojicic},
title = {Beyond Processor-centric Operating Systems},
booktitle = {15th Workshop on Hot Topics in Operating Systems (HotOS {XV})},
year = {2015},
address = {Kartause Ittingen, Switzerland},
url = {https://www.usenix.org/conference/hotos15/workshop-program/presentation/faraboschi},
publisher = {{USENIX} Association},
}

@inproceedings{hpcc, author = {Li, Yuliang and Miao, Rui and Liu, Hongqiang Harry and Zhuang, Yan and Feng, Fei and Tang, Lingbo and Cao, Zheng and Zhang, Ming and Kelly, Frank and Alizadeh, Mohammad and Yu, Minlan}, title = {HPCC: High Precision Congestion Control}, year = {2019}, isbn = {9781450359566}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341302.3342085}, doi = {10.1145/3341302.3342085}, abstract = {Congestion control (CC) is the key to achieving ultra-low latency, high bandwidth and network stability in high-speed networks. From years of experience operating large-scale and high-speed RDMA networks, we find the existing high-speed CC schemes have inherent limitations for reaching these goals. In this paper, we present HPCC (High Precision Congestion Control), a new high-speed CC mechanism which achieves the three goals simultaneously. HPCC leverages in-network telemetry (INT) to obtain precise link load information and controls traffic precisely. By addressing challenges such as delayed INT information during congestion and overreac-tion to INT information, HPCC can quickly converge to utilize free bandwidth while avoiding congestion, and can maintain near-zero in-network queues for ultra-low latency. HPCC is also fair and easy to deploy in hardware. We implement HPCC with commodity programmable NICs and switches. In our evaluation, compared to DCQCN and TIMELY, HPCC shortens flow completion times by up to 95%, causing little congestion even under large-scale incasts.}, booktitle = {Proceedings of the ACM Special Interest Group on Data Communication}, pages = {44–58}, numpages = {15}, keywords = {RDMA, congestion control, programmable switch, smart NIC}, location = {Beijing, China}, series = {SIGCOMM '19} }

@article{dcqcn, author = {Zhu, Yibo and Eran, Haggai and Firestone, Daniel and Guo, Chuanxiong and Lipshteyn, Marina and Liron, Yehonatan and Padhye, Jitendra and Raindel, Shachar and Yahia, Mohamad Haj and Zhang, Ming}, title = {Congestion Control for Large-Scale RDMA Deployments}, year = {2015}, issue_date = {October 2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {45}, number = {4}, issn = {0146-4833}, url = {https://doi.org/10.1145/2829988.2787484}, doi = {10.1145/2829988.2787484}, abstract = {Modern datacenter applications demand high throughput (40Gbps) and ultra-low latency (< 10 μs per hop) from the network, with low CPU overhead. Standard TCP/IP stacks cannot meet these requirements, but Remote Direct Memory Access (RDMA) can. On IP-routed datacenter networks, RDMA is deployed using RoCEv2 protocol, which relies on Priority-based Flow Control (PFC) to enable a drop-free network. However, PFC can lead to poor application performance due to problems like head-of-line blocking and unfairness. To alleviates these problems, we introduce DCQCN, an end-to-end congestion control scheme for RoCEv2. To optimize DCQCN performance, we build a fluid model, and provide guidelines for tuning switch buffer thresholds, and other protocol parameters. Using a 3-tier Clos network testbed, we show that DCQCN dramatically improves throughput and fairness of RoCEv2 RDMA traffic. DCQCN is implemented in Mellanox NICs, and is being deployed in Microsoft's datacenters.}, journal = {SIGCOMM Comput. Commun. Rev.}, month = {aug}, pages = {523–536}, numpages = {14}, keywords = {PFC, datacenter transport, RDMA, ECN, congestion control} }
 
 
@inproceedings{10.1145/2785956.2787484, author = {Zhu, Yibo and Eran, Haggai and Firestone, Daniel and Guo, Chuanxiong and Lipshteyn, Marina and Liron, Yehonatan and Padhye, Jitendra and Raindel, Shachar and Yahia, Mohamad Haj and Zhang, Ming}, title = {Congestion Control for Large-Scale RDMA Deployments}, year = {2015}, isbn = {9781450335423}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2785956.2787484}, doi = {10.1145/2785956.2787484}, abstract = {Modern datacenter applications demand high throughput (40Gbps) and ultra-low latency (< 10 μs per hop) from the network, with low CPU overhead. Standard TCP/IP stacks cannot meet these requirements, but Remote Direct Memory Access (RDMA) can. On IP-routed datacenter networks, RDMA is deployed using RoCEv2 protocol, which relies on Priority-based Flow Control (PFC) to enable a drop-free network. However, PFC can lead to poor application performance due to problems like head-of-line blocking and unfairness. To alleviates these problems, we introduce DCQCN, an end-to-end congestion control scheme for RoCEv2. To optimize DCQCN performance, we build a fluid model, and provide guidelines for tuning switch buffer thresholds, and other protocol parameters. Using a 3-tier Clos network testbed, we show that DCQCN dramatically improves throughput and fairness of RoCEv2 RDMA traffic. DCQCN is implemented in Mellanox NICs, and is being deployed in Microsoft's datacenters.}, booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication}, pages = {523–536}, numpages = {14}, keywords = {ECN, datacenter transport, congestion control, RDMA, PFC}, location = {London, United Kingdom}, series = {SIGCOMM '15} }

@inproceedings{mom, author = {Ports, Dan R. K. and Li, Jialin and Liu, Vincent and Sharma, Naveen Kr. and Krishnamurthy, Arvind}, title = {Designing Distributed Systems Using Approximate Synchrony in Data Center Networks}, year = {2015}, isbn = {9781931971218}, publisher = {USENIX Association}, address = {USA}, abstract = {Distributed systems are traditionally designed independently from the underlying network, making worst-case assumptions (e.g., complete asynchrony) about its behavior. However, many of today's distributed applications are deployed in data centers, where the network is more reliable, predictable, and extensible. In these environments, it is possible to co-design distributed systems with their network layer, and doing so can offer substantial benefits.This paper explores network-level mechanisms for providing Mostly-Ordered Multicast (MOM): a best-effort ordering property for concurrent multicast operations. Using this primitive, we design Speculative Paxos, a state machine replication protocol that relies on the network to order requests in the normal case. This approach leads to substantial performance benefits: under realistic data center conditions, Speculative Paxos can provide 40% lower latency and 2.6\texttimes{} higher throughput than the standard Paxos protocol. It offers lower latency than a latency-optimized protocol (Fast Paxos) with the same throughput as a throughput-optimized protocol (batching).}, booktitle = {Proceedings of the 12th USENIX Conference on Networked Systems Design and Implementation}, pages = {43–57}, numpages = {15}, location = {Oakland, CA}, series = {NSDI'15} }

@inproceedings{helios,
author = {Nightingale, Edmund B and Hodson, Orion and McIlroy, Ross and Hawblitzel, Chris and Hunt, Galen},
title = {Helios: Heterogeneous Multiprocessing with Satellite Kernels},
booktitle = {Proceedings of the 22nd Symposium on Operating Systems Principles (SOSP '09)},
year = {2009},
month = {October},
abstract = {
Helios is an operating system designed to simplify the task of writing,
deploying, and tuning applications for heterogeneous platforms. Helios
    introduces satellite kernels, which export a single, uniform set of OS
    abstractions across CPUs of disparate architectures and performance
    characteristics. Access to I/O services such as ﬁle systems are made
    transparent via remote message passing, which extends a standard
    microkernel message-passing abstraction to a satellite kernel
    infrastructure. Helios retargets applications to available ISAs by
    compiling froman intermediate language. To simplify deploying and tuning
    application performance, Helios exposes an afﬁnity metric to developers.
    Afﬁnity provides a hint to the operating system about whether a process
    would beneﬁt from executing on the same platform as a service it depends
    upon.  We developed satellite kernels for an XScale programmable I/O card
    and for cache-coherent NUMA architectures. We ofﬂoaded several applications
    and operating system components, often by changing only a single line of
    metadata. We show up to a 28% performance improvement by ofﬂoading tasks to
    the XScale I/O card. On a mail-server benchmark, we show a 39% improvement
    in performance by automatically splitting the application among multiple
    NUMA domains.
},
publisher = {Association for Computing Machinery, Inc.},
url = {https://www.microsoft.com/en-us/research/publication/helios-heterogeneous-multiprocessing-with-satellite-kernels/},
edition = {Proceedings of the 22nd Symposium on Operating Systems Principles (SOSP '09)},
}

@inproceedings{zombieland,
 author = {Nitu, Vlad and Teabe, Boris and Tchana, Alain and Isci, Canturk and Hagimont, Daniel},
 title = "Welcome to {Zombieland}: Practical and Energy-efficient Memory Disaggregation in a Datacenter",
 booktitle = {Proceedings of the Thirteenth EuroSys Conference},
 series = {EuroSys '18},
 year = {2018},
 isbn = {978-1-4503-5584-1},
 location = {Porto, Portugal},
 pages = {16:1--16:12},
 articleno = {16},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/3190508.3190537},
 doi = {10.1145/3190508.3190537},
 acmid = {3190537},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy efficiency, memory disaggregation, virtualization},
} 

@inproceedings{Schroeder:2007:DFR:1267903.1267904,
 author = {Schroeder, Bianca and Gibson, Garth A.},
 title = {Disk Failures in the Real World: What Does an MTTF of 1,000,000 Hours Mean to You?},
 booktitle = {Proceedings of the 5th USENIX Conference on File and Storage Technologies},
 series = {FAST '07},
 year = {2007},
 location = {San Jose, CA},
 articleno = {1},
 url = {http://dl.acm.org/citation.cfm?id=1267903.1267904},
 acmid = {1267904},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
}

@inproceedings {cachecloud,
author = {Shelby Thomas and Geoffrey M. Voelker and George Porter},
title = {CacheCloud: Towards Speed-of-light Datacenter Communication},
booktitle = {10th {USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud 18)},
year = {2018},
address = {Boston, MA},
url = {https://www.usenix.org/conference/hotcloud18/presentation/thomas},
publisher = {{USENIX} Association},
}


@inproceedings {legoos,
author = {Yizhou Shan and Yutong Huang and Yilun Chen and Yiying Zhang},
title = "{LegoOS}: A Disseminated, Distributed {OS} for Hardware Resource Disaggregation",
booktitle = {13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
year = {2018},
isbn = {978-1-931971-47-8},
address = {Carlsbad, CA},
pages = {69--87},
url = {https://www.usenix.org/conference/osdi18/presentation/shan},
publisher = {{USENIX} Association},
}

@inproceedings{the-multikernel,
author = {Baumann, Andrew and Barham, Paul and Isaacs, Rebecca and Harris, Tim},
title = {The Multikernel: A new OS architecture for scalable multicore systems},
booktitle = {22nd Symposium on Operating Systems Principles},
year = {2009},
month = {October},
abstract = {Commodity computer systems contain more and more processor cores and exhibit increasingly diverse architectural tradeoffs, including memory hierarchies, interconnects, instruction sets and variants, and IO configurations. Previous high-performance computing systems have scaled in specific cases, but the dynamic nature of modern client and server workloads, coupled with the impossibility of statically optimizing an OS for all workloads and hardware variants pose serious challenges for operating system structures.

We argue that the challenge of future multicore hardware is best met by embracing the networked nature of the machine, rethinking OS architecture using ideas from distributed systems. We investigate a new OS structure, the multikernel, that treats the machine as a network of independent cores, assumes no inter-core sharing at the lowest level, and moves traditional OS functionality to a distributed system of processes that communicate via message-passing.

We have implemented a multikernel OS to show that the approach is promising, and we describe how traditional scalability problems for operating systems (such as memory management) can be effectively recast using messages and can exploit insights from distributed systems and networking.  An evaluation of our prototype on multicore systems shows that, even on present-day machines, the performance of a multikernel is comparable with a conventional OS, and can scale better to support future hardware.

},
publisher = {Association for Computing Machinery, Inc.},
url = {https://www.microsoft.com/en-us/research/publication/the-multikernel-a-new-os-architecture-for-scalable-multicore-systems/},
edition = {22nd Symposium on Operating Systems Principles},
}

@inproceedings {clover,
author = {Shin-Yeh Tsai and Yizhou Shan and Yiying Zhang},
title = {Disaggregating Persistent Memory and Controlling Them Remotely: An Exploration of Passive Disaggregated Key-Value Stores},
booktitle = "{USENIX} Annual Technical Conference",
year = {2020},
isbn = {978-1-939133-14-4},
pages = {33--48},
url = {https://www.usenix.org/conference/atc20/presentation/tsai},
month = jul,
}

@article{storm,
  author    = {Stanko Novakovic and
               Yizhou Shan and
               Aasheesh Kolli and
               Michael Cui and
               Yiying Zhang and
               Haggai Eran and
               Liran Liss and
               Michael Wei and
               Dan Tsafrir and
               Marcos K. Aguilera},
  title     = {Storm: a fast transactional dataplane for remote data structures},
  journal   = {CoRR},
  volume    = {abs/1902.02411},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.02411},
  archivePrefix = {arXiv},
  eprint    = {1902.02411},
  timestamp = {Tue, 21 May 2019 18:03:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-02411.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lite,
author = {Tsai, Shin-Yeh and Zhang, Yiying},
title = "{LITE} Kernel {RDMA} Support for Datacenter Applications",
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3132747.3132762},
doi = {10.1145/3132747.3132762},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {306–324},
numpages = {19},
keywords = {RDMA, indirection, low-latency network, network stack},
address = {Shanghai, China},
}

@InProceedings{aguilera2019designing,
author = {Aguilera, Marcos and Keeton, Kimberly and Novakovic, Stanko and Singhal, Sharad},
title = {Designing Far Memory Data Structures: Think Outside the Box},
organization = {ACM},
booktitle = {17th Workshop on Hot Topics in Operating Systems (HotOS)},
year = {2019},
month = {May},
abstract = {Technologies like RDMA and Gen-Z, which give access to memory outside the box, are gaining in popularity. These technologies provide the abstraction of far memory, where memory is attached to the network and can be accessed by remote processors without mediation by a local processor. Unfortunately, far memory is hard to use because existing data structures are mismatched to it. We argue that we need new data structures for far memory, borrowing techniques from concurrent data structures and distributed systems. We examine the requirements of these data structures and show how to realize them using simple hardware extensions},
url = {https://www.microsoft.com/en-us/research/publication/designing-far-memory-data-structures-think-outside-the-box/},
}

@inproceedings{surf,
author = {Zhang, Huanchen and Lim, Hyeontaek and Leis, Viktor and Andersen, David G. and Kaminsky, Michael and Keeton, Kimberly and Pavlo, Andrew},
title = {SuRF: Practical Range Query Filtering with Fast Succinct Tries},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3196931},
doi = {10.1145/3183713.3196931},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {323–336},
numpages = {14},
keywords = {surf, range filter, fast succinct tries, lsm-trees, succinct data structures},
location = {Houston, TX, USA},
series = {SIGMOD ’18}
}

@inproceedings{10.1145/3342195.3387522,
author = {Amaro, Emmanuel and Branner-Augmon, Christopher and Luo, Zhihong and Ousterhout, Amy and Aguilera, Marcos K. and Panda, Aurojit and Ratnasamy, Sylvia and Shenker, Scott},
title = {Can Far Memory Improve Job Throughput?},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387522},
doi = {10.1145/3342195.3387522},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {14},
numpages = {16},
location = {Heraklion, Greece},
series = {EuroSys ’20}
}

@inproceedings {reigons,
author = {Marcos K. Aguilera and Nadav Amit and Irina Calciu and Xavier Deguillard and Jayneel Gandhi and Stanko Novakovi{\'c} and Arun Ramanathan and Pratap Subrahmanyam and Lalith Suresh and Kiran Tati and Rajesh Venkatasubramanian and Michael Wei},
title = {Remote regions: a simple abstraction for remote memory},
booktitle = {2018 {USENIX} Annual Technical Conference ({USENIX} {ATC} 18)},
year = {2018},
isbn = {978-1-939133-01-4},
address = {Boston, MA},
pages = {775--787},
url = {https://www.usenix.org/conference/atc18/presentation/aguilera},
publisher = {{USENIX} Association},
month = jul,
}
 
@inproceedings {cell,
author = {Christopher Mitchell and Kate Montgomery and Lamont Nelson and Siddhartha Sen and Jinyang Li},
title = "Balancing {CPU} and Network in the Cell Distributed {B-Tree} Store",
booktitle = {2016 {USENIX} Annual Technical Conference ({USENIX} {ATC} 16)},
year = {2016},
isbn = {978-1-931971-30-0},
address = {Denver, CO},
pages = {451--464},
url = {https://www.usenix.org/conference/atc16/technical-sessions/presentation/mitchell},
publisher = {{USENIX} Association},
month = jun,
}

@inproceedings {pilaf,
author = {Christopher Mitchell and Yifeng Geng and Jinyang Li},
title = "Using One-Sided {RDMA} Reads to Build a Fast, {CPU}-Efficient Key-Value Store",
booktitle = {2013 {USENIX} Annual Technical Conference ({USENIX} {ATC} 13)},
year = {2013},
isbn = {978-1-931971-01-0},
address = {San Jose, CA},
pages = {103--114},
url = {https://www.usenix.org/conference/atc13/technical-sessions/presentation/mitchell},
publisher = {{USENIX} Association},
month = jun,
}

@inproceedings {254120,
author = {Sebastian Angel and Mihir Nanavati and Siddhartha Sen},
title = {Disaggregation and the Application},
booktitle = {12th {USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud 20)},
year = {2020},
url = {https://www.usenix.org/conference/hotcloud20/presentation/angel},
publisher = {{USENIX} Association},
month = jul,
}

@article{10.1145/224057.224072,
author = {Feeley, M. J. and Morgan, W. E. and Pighin, E. P. and Karlin, A. R. and Levy, H. M. and Thekkath, C. A.},
title = {Implementing Global Memory Management in a Workstation Cluster},
year = {1995},
issue_date = {Dec. 3, 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {5},
issn = {0163-5980},
url = {https://doi.org/10.1145/224057.224072},
doi = {10.1145/224057.224072},
journal = {SIGOPS Oper. Syst. Rev.},
month = dec,
pages = {201–212},
numpages = {12}
}

  

@inproceedings{gms,
author = {Feeley, M. J. and Morgan, W. E. and Pighin, E. P. and Karlin, A. R. and Levy, H. M. and Thekkath, C. A.},
title = {Implementing Global Memory Management in a Workstation Cluster},
year = {1995},
isbn = {0897917154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/224056.224072},
doi = {10.1145/224056.224072},
booktitle = {Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles},
pages = {201–212},
numpages = {12},
location = {Copper Mountain, Colorado, USA},
series = {SOSP '95}
}

@inproceedings {memc3,
author = {Bin Fan and David G. Andersen and Michael Kaminsky},
title = "{MemC3}: Compact and Concurrent MemCache with Dumber Caching and Smarter Hashing",
booktitle = {10th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 13)},
year = {2013},
isbn = {978-1-931971-00-3},
address = {Lombard, IL},
pages = {371--384},
url = {https://www.usenix.org/conference/nsdi13/technical-sessions/presentation/fan},
publisher = {{USENIX} Association},
month = apr,
}

@inproceedings{sonuma,
author = {Novakovic, Stanko and Daglis, Alexandros and Bugnion, Edouard and Falsafi, Babak and Grot, Boris},
title = {Scale-out NUMA},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541965},
doi = {10.1145/2541940.2541965},
abstract = {Emerging datacenter applications operate on vast datasets that are kept in DRAM to minimize latency. The large number of servers needed to accommodate this massive memory footprint requires frequent server-to-server communication in applications such as key-value stores and graph-based applications that rely on large irregular data structures. The fine-grained nature of the accesses is a poor match to commodity networking technologies, including RDMA, which incur delays of 10-1000x over local DRAM operations. We introduce Scale-Out NUMA (soNUMA) -- an architecture, programming model, and communication protocol for low-latency, distributed in-memory processing. soNUMA layers an RDMA-inspired programming model directly on top of a NUMA memory fabric via a stateless messaging protocol. To facilitate interactions between the application, OS, and the fabric, soNUMA relies on the remote memory controller -- a new architecturally-exposed hardware block integrated into the node's local coherence hierarchy. Our results based on cycle-accurate full-system simulation show that soNUMA performs remote reads at latencies that are within 4x of local DRAM, can fully utilize the available memory bandwidth, and can issue up to 10M remote memory operations per second per core.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {3–18},
numpages = {16},
keywords = {numa, system-on-chips, rmda},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@inproceedings{amanda-hotnets,
author = {Carbonari, Amanda and Beschasnikh, Ivan},
title = {Tolerating Faults in Disaggregated Datacenters},
year = {2017},
isbn = {9781450355698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152434.3152447},
doi = {10.1145/3152434.3152447},
abstract = {Recent research shows that disaggregated datacenters (DDCs) are practical and that DDC resource modularity will benefit both users and operators. This paper explores the implications of disaggregation on application fault tolerance. We expect that resource failures in a DDC will be fine-grained because resources will no longer fate-share. In this context, we look at how DDCs can provide legacy applications with familiar failure semantics and discuss fate sharing granularities that are not available in existing datacenters. We argue that fate sharing and failure mitigation should be programmable, specified by the application, and primarily implemented in the SDN-based network.},
booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
pages = {164–170},
numpages = {7},
location = {Palo Alto, CA, USA},
series = {HotNets-XVI}
}

@inproceedings {erpc,
author = {Anuj Kalia and Michael Kaminsky and David Andersen},
title = "Datacenter {RPCs} can be General and Fast",
booktitle = {16th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 19)},
year = {2019},
isbn = {978-1-931971-49-2},
address = {Boston, MA},
pages = {1--16},
url = {https://www.usenix.org/conference/nsdi19/presentation/kalia},
publisher = {{USENIX} Association},
month = feb,
}

@inproceedings {farm,
author = {Aleksandar Dragojevi{\'c} and Dushyanth Narayanan and Miguel Castro and Orion Hodson},
title = "{FaRM}: Fast Remote Memory",
booktitle = {11th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 14)},
year = {2014},
isbn = {978-1-931971-09-6},
address = {Seattle, WA},
pages = {401--414},
url = {https://www.usenix.org/conference/nsdi14/technical-sessions/dragojevi{\'c}},
publisher = {{USENIX} Association},
month = apr,
}

@inproceedings {fasst,
author = {Anuj Kalia and Michael Kaminsky and David G. Andersen},
title = "{FaSST}: Fast, Scalable and Simple Distributed Transactions with Two-Sided ({RDMA}) Datagram {RPCs}",
booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {185--201},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/kalia},
publisher = {{USENIX} Association},
month = nov,
}

@inproceedings {design-guidelines,
author = "Anuj Kalia and Michael Kaminsky and David G. Andersen",
title = "Design Guidelines for High Performance {RDMA} Systems",
booktitle = {2016 {USENIX} Annual Technical Conference ({USENIX} {ATC} 16)},
year = {2016},
isbn = {978-1-931971-30-0},
address = {Denver, CO},
pages = {437--450},
url = {https://www.usenix.org/conference/atc16/technical-sessions/presentation/kalia},
publisher = {{USENIX} Association},
month = jun,
}

@inproceedings {infiniswap,
author = {Juncheng Gu and Youngmoon Lee and Yiwen Zhang and Mosharaf Chowdhury and Kang G. Shin},
title = {Efficient Memory Disaggregation with Infiniswap},
booktitle = {14th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 17)},
year = {2017},
isbn = {978-1-931971-37-9},
address = {Boston, MA},
pages = {649--667},
url = {https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/gu},
publisher = {{USENIX} Association},
month = mar,
}
@inproceedings {leap,
author = {Hasan Al Maruf and Mosharaf Chowdhury},
title = "Effectively Prefetching Remote Memory with {Leap}",
booktitle = {2020 {USENIX} Annual Technical Conference ({USENIX} {ATC} 20)},
year = {2020},
isbn = {978-1-939133-14-4},
pages = {843--857},
url = {https://www.usenix.org/conference/atc20/presentation/al-maruf},
publisher = {{USENIX} Association},
month = jul,
}


@article{herd,
author = {Kalia, Anuj and Kaminsky, Michael and Andersen, David G.},
title = {Using RDMA Efficiently for Key-Value Services},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2740070.2626299},
doi = {10.1145/2740070.2626299},
abstract = {This paper describes the design and implementation of HERD, a key-value system designed to make the best use of an RDMA network. Unlike prior RDMA-based key-value systems, HERD focuses its design on reducing network round trips while using efficient RDMA primitives; the result is substantially lower latency, and throughput that saturates modern, commodity RDMA hardware.HERD has two unconventional decisions: First, it does not use RDMA reads, despite the allure of operations that bypass the remote CPU entirely. Second, it uses a mix of RDMA and messaging verbs, despite the conventional wisdom that the messaging primitives are slow. A HERD client writes its request into the server's memory; the server computes the reply. This design uses a single round trip for all requests and supports up to 26 million key-value operations per second with 5μs average latency. Notably, for small key-value items, our full system throughput is similar to native RDMA read throughput and is over 2X higher than recent RDMA-based key-value systems. We believe that HERD further serves as an effective template for the construction of RDMA-based datacenter services.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {295–306},
numpages = {12},
keywords = {infiniband, ROCE, key-value stores, RDMA}
}

@inproceedings {requirements,
author = {Peter X. Gao and Akshay Narayan and Sagar Karandikar and Joao Carreira and Sangjin Han and Rachit Agarwal and Sylvia Ratnasamy and Scott Shenker},
title = {Network Requirements for Resource Disaggregation},
booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {249--264},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/gao},
publisher = {{USENIX} Association},
month = nov,
}  

@inproceedings {disandapp,
author = {Sebastian Angel and Mihir Nanavati and Siddhartha Sen},
title = {Disaggregation and the Application},
booktitle = {12th {USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud 20)},
year = {2020},
url = {https://www.usenix.org/conference/hotcloud20/presentation/angel},
publisher = {{USENIX} Association},
month = jul,
}

@article{Mansour_2019,
   title="{FPGA} Implementation of {RDMA-Based} Data Acquisition System Over {100-Gb Ethernet}",
   volume={66},
   ISSN={1558-1578},
   url={http://dx.doi.org/10.1109/TNS.2019.2904118},
   DOI={10.1109/tns.2019.2904118},
   number={7},
   journal={IEEE Transactions on Nuclear Science},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Mansour, Wassim and Janvier, Nicolas and Fajardo, Pablo},
   year={2019},
   month={Jul},
   pages={1138–1143}
}

@inproceedings{fastswap,
author = {Amaro, Emmanuel and Branner-Augmon, Christopher and Luo, Zhihong and Ousterhout, Amy and Aguilera, Marcos K. and Panda, Aurojit and Ratnasamy, Sylvia and Shenker, Scott},
title = {Can Far Memory Improve Job Throughput?},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387522},
doi = {10.1145/3342195.3387522},
abstract = {As memory requirements grow, and advances in memory technology slow, the availability of sufficient main memory is increasingly the bottleneck in large compute clusters. One solution to this is memory disaggregation, where jobs can remotely access memory on other servers, or far memory. This paper first presents faster swapping mechanisms and a far memory-aware cluster scheduler that make it possible to support far memory at rack scale. Then, it examines the conditions under which this use of far memory can increase job throughput. We find that while far memory is not a panacea, for memory-intensive workloads it can provide performance improvements on the order of 10% or more even without changing the total amount of memory available.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {14},
numpages = {16},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@MISC{tofino2,
    title = "Intel Tofino 2 P4 Programmability with More Bandwidth",
    howpublished = "\url{https://www.intel.com/content/www/us/en/products/network-io/programmable-ethernet-switch/tofino-2-series/tofino-2.html}",
    year = 2020
}

@misc{rfc3135,
    series =    {Request for Comments},
    number =    3135,
    howpublished =  {RFC 3135},
    publisher = {RFC Editor},
    doi =       {10.17487/RFC3135},
    url =       {https://www.rfc-editor.org/info/rfc3135},
        author =    {Jim Griner and John Border and Markku Kojo and Zach D. Shelby and Gabriel Montenegro},
    title =     {{Performance Enhancing Proxies Intended to Mitigate Link-Related Degradations}},
    pagetotal = 45,
    year =      2001,
    month =     jun,
    abstract =  {This document is a survey of Performance Enhancing Proxies (PEPs) often employed to improve degraded TCP performance caused by characteristics of specific link environments, for example, in satellite, wireless WAN, and wireless LAN environments. This memo provides information for the Internet community.},
}
@inproceedings{snoop, author = {Balakrishnan, Hari and Seshan, Srinivasan and Amir, Elan and Katz, Randy H.}, title = {Improving TCP/IP Performance over Wireless Networks}, year = {1995}, isbn = {0897918142}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/215530.215544}, doi = {10.1145/215530.215544}, abstract = {TCP is a reliable transport protocol tuned to perform well in traditional networks made up of links with low bit-error rates. Networks with higher bit-error rates, such as those with wireless links and mobile hosts, violate many of the assumptions made by TCP, causing degraded end-to-end performance. In tbis paper, we describe the design and implementation of a simple protocol, called the snoop protocol, that improves TCP performance in wireless networks. The protocol modifies network-layer software mainly at a base station and preserves end-to-end TCP semantics. The main idea of the protocol is to cache packets at the base station and perform local retransmissions across the wireless link. We have implemented the snoop protocol on a wireless testbed consisting of IBM ThinkPad laptops and i486 base stations communicating over an AT&T Wavelan. Our experiments show that it is significantly more robust at dealing with unreliable wireless links as compared to normal TCP; we have achieved throughput speedups of up to 20 times over regular TCP in our experiments with the protocol.}, booktitle = {Proceedings of the 1st Annual International Conference on Mobile Computing and Networking}, pages = {2–11}, numpages = {10}, location = {Berkeley, California, USA}, series = {MobiCom '95} }

@misc{Grant2021InContRes,
  author = "Anonymous",
  title = "Anonymized workshop paper",
  }
  
@Comment author = {Grant, Stewart and Snoeren, Alex C.},
@Comment title = {In-network Contention Resolution for Disaggregated Memory},
@Comment booktitle = {Proceedings of the Second Workshop on Disaggregation and Serverless (WORDS'21)},
@Comment year = {2021},
@Comment url = {https://wuklab.github.io/words/words21-grant.pdf},
@Comment numpages = {7},
@Comment location = {Virtual Event, USA},
@Comment series = {WORDS '21}
@Comment }

@MISC{genz,
  author = {GenZ},
  title = "{Gen-Z Consortium}",
  howpublished = {https://genzconsortium.org/},
  year = {2018}
}

@MISC{omni-path,
  author = {Intel},
  title = {Intel Omni-Path Architecture},
  howpublished = {https://tinyurl.com/ya3x4ktd},
}


@MISC{connectx,
  author = "{NVIDIA}",
  title = "{ConnectX} {SmartNICs}",
  howpublished = {https://www.nvidia.com/en-us/networking/ethernet-adapters/},
  }

@MISC{cavium,
    author = {Cavium},
    title = "Liquid {IO II} 10/25G Smart {NIC} Family",
    howpublished = {https://www.marvell.com/documents/08icqisgkbtn6kstgzh4/},
    year = 2017
}

@MISC{fungible,
  author = {Fungible},
  title = "The {Fungible} Data Processing Unit",
  howpublished = {https://www.fungible.com/product/dpu-platform/},
  year = 2021,
}

@misc{clio-arxiv,
      title={Clio: A Hardware-Software Co-Designed Disaggregated Memory System}, 
      author={Zhiyuan Guo and Yizhou Shan and Xuhao Luo and Yutong Huang and Yiying Zhang},
      year={2021},
      eprint={2108.03492},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@inproceedings{corundum,
	title="Corundum: An Open-Source {100-Gbps} {NIC}",
	author="Alex {Forencich} and Alex C. {Snoeren} and George {Porter} and George {Papen}",
	booktitle="2020 IEEE 28th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)",
	pages="38--46",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3034977853",
	year="2020"
}

@InProceedings{kv-direct,
author = {Li, Bojie and Ruan, Zhenyuan and Xiao, Wencong and Lu, Yuanwei and Xiong, Yongqiang and Putnam, Andrew and Chen, Enhong and Zhang, Lintao},
title = {KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
year = {2017},
month = {October},
abstract = {Performance of in-memory key-value store (KVS) continues to be of great importance as modern KVS goes beyond the traditional object-caching workload and becomes a key infrastructure to support distributed main-memory computation in data centers. Recent years have witnessed a rapid increase of network bandwidth in data centers, shifting the bottleneck of most KVS from the network to the CPU. RDMA-capable NIC partly alleviates the problem, but the primitives provided by RDMA abstraction are rather limited. Meanwhile, programmable NICs become available in data centers, enabling in-network processing. In this paper, we present KV-Direct, a high performance KVS that leverages programmable NIC to extend RDMA primitives and enable remote direct key-value access to the main host memory.

We develop several novel techniques to maximize the throughput and hide the latency of the PCIe connection between the NIC and the host memory, which becomes the new bottleneck. Combined, these mechanisms allow a single NIC KV-Direct to achieve up to 180 M key-value operations per second, equivalent to the throughput of tens of CPU cores. Compared with CPU based KVS implementation, KV-Direct improves power efficiency by 3x, while keeping tail latency below 10 μs. Moreover, KV-Direct can achieve near linear scalability with multiple NICs. With 10 programmable NIC cards in a commodity server, we achieve 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store.},
publisher = {ACM},
url = {https://www.microsoft.com/en-us/research/publication/kv-direct-high-performance-memory-key-value-store-programmable-nic/},
pages = {137-152},
isbn = {978-1-4503-5085-3},
edition = {Proceedings of the 26th Symposium on Operating Systems Principles},
}



@inproceedings {decible,
author = {Mihir Nanavati and Jake Wires and Andrew Warfield},
title = {Decibel: Isolation and Sharing in Disaggregated Rack-Scale Storage},
booktitle = {14th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 17)},
year = {2017},
isbn = {978-1-931971-37-9},
address = {Boston, MA},
pages = {17--33},
url = {https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/nanavati},
publisher = {{USENIX} Association},
month = mar,
}

@inproceedings {aifm,
author = {Zhenyuan Ruan and Malte Schwarzkopf and Marcos K. Aguilera and Adam Belay},
title = {{AIFM}: High-Performance, Application-Integrated Far Memory},
booktitle = {14th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {315--332},
url = {https://www.usenix.org/conference/osdi20/presentation/ruan},
publisher = {{USENIX} Association},
month = nov,
}

@inproceedings{firebox,
author = {Karandikar, Sagar and Ou, Albert and Amid, Alon and Mao, Howard and Katz, Randy and Nikoli\'{c}, Borivoje and Asanovi\'{c}, Krste},
title = "{FirePerf}: {FPGA}-Accelerated Full-System Hardware/Software Performance Profiling and Co-Design",
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378455},
doi = {10.1145/3373376.3378455},
abstract = {Achieving high-performance when developing specialized hardware/software systems requires
understanding and improving not only core compute kernels, but also intricate and
elusive system-level bottlenecks. Profiling these bottlenecks requires both high-fidelity
introspection and the ability to run sufficiently many cycles to execute complex software
stacks, a challenging combination. In this work, we enable agile full-system performance
optimization for hardware/software systems with FirePerf, a set of novel out-of-band
system-level performance profiling capabilities integrated into the open-source FireSim
FPGA-accelerated hardware simulation platform. Using out-of-band call stack reconstruction
and automatic performance counter insertion, FirePerf enables introspecting into hardware
and software at appropriate abstraction levels to rapidly identify opportunities for
software optimization and hardware specialization, without disrupting end-to-end system
behavior like traditional profiling tools. We demonstrate the capabilities of FirePerf
with a case study that optimizes the hardware/software stack of an open-source RISC-V
SoC with an Ethernet NIC to achieve 8x end-to-end improvement in achievable bandwidth
for networking applications running on Linux. We also deploy a RISC-V Linux kernel
optimization discovered with FirePerf on commercial RISC-V silicon, resulting in up
to 1.72x improvement in network performance.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {715–731},
numpages = {17},
keywords = {fpga-accelerated simulation, network performance optimization, performance profiling, agile hardware, hardware/software co-design},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@inproceedings {ivy,
author = {Athicha Muthitacharoen and Robert Morris and Thomer M. Gil and Benjie Chen},
title = {Ivy: A Read/Write Peer-to-Peer File System},
booktitle = {5th Symposium on Operating Systems Design and Implementation ({OSDI} 02)},
year = {2002},
address = {Boston, MA},
url = {https://www.usenix.org/conference/osdi-02/ivy-readwrite-peer-peer-file-system},
publisher = {{USENIX} Association},
month = dec,
}

@misc{infiniband-spec,
  author = "{Infiniband Trade Association}",
  title = {Infiniband Specification},
  howpublished = {https://www.afs.enea.it/asantoro/},
  year = {2007},
}

, fastswap

@misc{intel-rack,
  author = {Intel},
  title = "Intel Rack Scale Architecture: Faster Service Delivery and Lower {TCO}",
  howpublished = {https://www.intel.com/content/www/us/en/architecture-and-technology/rack-scale-design-overview.html}
}

@misc{cxl,
  author = "{CXL Consortium}",
  title = "{CXL} 3.0 Specification",
  howpublished = {https://www.computeexpresslink.org/download-the-specification},
}

@inproceedings{cliquemap,
author = "Singhvi, Arjun and Akella, Aditya and Anderson, Maggie and Cauble, Rob and Deshmukh, Harshad and Gibson, Dan and Martin, Milo M. K. and Strominger, Amanda and Wenisch, Thomas F. and Vahdat, Amin",
title = "CliqueMap: Productionizing an {RMA-Based} Distributed Caching System",
year = {2021},
publisher = {Association for Computing Machinery},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference}, 
pages = {93–105}, 
numpages = {13}, 
 location = {Virtual Event, USA}, 
series = {SIGCOMM '21}
}




@misc{supernic,
      title={Disaggregating and Consolidating Network Functionalities}, 
      author={Yizhou Shan and Will Lin and Ryan Kosta and Arvind Krishnamurthy and Yiying Zhang},
      year={2021},
      eprint={2109.07744},
      howpublished={arXiv},
      primaryClass={cs.DC}
}

@inproceedings{rethinking,
author = {Calciu, Irina and Imran, M. Talha and Puddu, Ivan and Kashyap, Sanidhya and Maruf, Hasan Al and Mutlu, Onur and Kolli, Aasheesh},
title = {Rethinking Software Runtimes for Disaggregated Memory},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446713},
doi = {10.1145/3445814.3446713},
abstract = {Disaggregated memory can address resource provisioning inefficiencies in current datacenters. Multiple software runtimes for disaggregated memory have been proposed in an attempt to make disaggregated memory practical. These systems rely on the virtual memory subsystem to transparently offer disaggregated memory to applications using a local memory abstraction. Unfortunately, using virtual memory for disaggregation has multiple limitations, including high overhead that comes from the use of page faults to identify what data to fetch and cache locally, and high dirty data amplification that comes from the use of page-granularity for tracking changes to the cached data (4KB or higher).  In this paper, we propose a fundamentally new approach to designing software runtimes for disaggregated memory that addresses these limitations. Our main observation is that we can use cache coherence instead of virtual memory for tracking applications' memory accesses transparently, at cache-line granularity. This simple idea (1) eliminates page faults from the application critical path when accessing remote data, and (2) decouples the application memory access tracking from the virtual memory page size, enabling cache-line granularity dirty data tracking and eviction. Using this observation, we implemented a new software runtime for disaggregated memory that improves average memory access time by 1.7-5X and reduces dirty data amplification by 2-10X, compared to state-of-the-art systems.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {79–92},
numpages = {14},
keywords = {remote memory, disaggregated memory, cache coherence},
location = {Virtual, USA},
series = {ASPLOS 2021}
}

@article{ramcloud,
author = {Ousterhout, John and Gopalan, Arjun and Gupta, Ashish and Kejriwal, Ankita and Lee, Collin and Montazeri, Behnam and Ongaro, Diego and Park, Seo Jin and Qin, Henry and Rosenblum, Mendel and Rumble, Stephen and Stutsman, Ryan and Yang, Stephen},
title = {The RAMCloud Storage System},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {0734-2071},
url = {https://doi.org/10.1145/2806887},
doi = {10.1145/2806887},
abstract = {RAMCloud is a storage system that provides low-latency access to large-scale datasets. To achieve low latency, RAMCloud stores all data in DRAM at all times. To support large capacities (1PB or more), it aggregates the memories of thousands of servers into a single coherent key-value store. RAMCloud ensures the durability of DRAM-based data by keeping backup copies on secondary storage. It uses a uniform log-structured mechanism to manage both DRAM and secondary storage, which results in high performance and efficient memory usage. RAMCloud uses a polling-based approach to communication, bypassing the kernel to communicate directly with NICs; with this approach, client applications can read small objects from any RAMCloud storage server in less than 5μs, durable writes of small objects take about 13.5μs. RAMCloud does not keep multiple copies of data online; instead, it provides high availability by recovering from crashes very quickly (1 to 2 seconds). RAMCloud’s crash recovery mechanism harnesses the resources of the entire cluster working concurrently so that recovery performance scales with cluster size.},
journal = {ACM Trans. Comput. Syst.},
month = {aug},
articleno = {7},
numpages = {55},
keywords = {storage systems, low latency, large-scale systems, Datacenters}
}

@inproceedings{dredbox,
  author={Bielski, M. and Syrigos, I. and Katrinis, K. and Syrivelis, D. and Reale, A. and Theodoropoulos, D. and Alachiotis, N. and Pnevmatikatos, D. and Pap, E.H. and Zervas, G. and Mishra, V. and Saljoghei, A. and Rigo, A. and Zazo, J. Fernando and Lopez-Buedo, S. and Torrents, M. and Zyulkyarov, F. and Enrico, M. and de Dios, O. Gonzalez},
  booktitle={2018 Design, Automation   Test in Europe Conference   Exhibition (DATE)}, 
  title={dReDBox: Materializing a full-stack rack-scale system prototype of a next-generation disaggregated datacenter}, 
  year={2018},
  volume={},
  number={},
  pages={1093-1098},
  doi={10.23919/DATE.2018.8342174}}

@inproceedings{ycsb,
author = {Cooper, Brian F. and Silberstein, Adam and Tam, Erwin and Ramakrishnan, Raghu and Sears, Russell},
title = {Benchmarking Cloud Serving Systems with YCSB},
year = {2010},
isbn = {9781450300360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1807128.1807152},
doi = {10.1145/1807128.1807152},
abstract = {While the use of MapReduce systems (such as Hadoop) for large scale data analysis has been widely recognized and studied, we have recently seen an explosion in the number of systems developed for cloud data serving. These newer systems address "cloud OLTP" applications, though they typically do not support ACID transactions. Examples of systems proposed for cloud serving use include BigTable, PNUTS, Cassandra, HBase, Azure, CouchDB, SimpleDB, Voldemort, and many others. Further, they are being applied to a diverse range of applications that differ considerably from traditional (e.g., TPC-C like) serving workloads. The number of emerging cloud serving systems and the wide range of proposed applications, coupled with a lack of apples-to-apples performance comparisons, makes it difficult to understand the tradeoffs between systems and the workloads for which they are suited. We present the "Yahoo! Cloud Serving Benchmark" (YCSB) framework, with the goal of facilitating performance comparisons of the new generation of cloud data serving systems. We define a core set of benchmarks and report results for four widely used systems: Cassandra, HBase, Yahoo!'s PNUTS, and a simple sharded MySQL implementation. We also hope to foster the development of additional cloud benchmark suites that represent other classes of applications by making our benchmark tool available via open source. In this regard, a key feature of the YCSB framework/tool is that it is extensible--it supports easy definition of new workloads, in addition to making it easy to benchmark new systems.},
booktitle = {Proceedings of the 1st ACM Symposium on Cloud Computing},
pages = {143–154},
numpages = {12},
keywords = {cloud serving database, benchmarking},
location = {Indianapolis, Indiana, USA},
series = {SoCC '10}
}

  
@inproceedings{blade-server,
author = {Lim, Kevin and Chang, Jichuan and Mudge, Trevor and Ranganathan, Parthasarathy and Reinhardt, Steven K. and Wenisch, Thomas F.},
title = {Disaggregated Memory for Expansion and Sharing in Blade Servers},
year = {2009},
isbn = {9781605585260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1555754.1555789},
doi = {10.1145/1555754.1555789},
abstract = {Analysis of technology and application trends reveals a growing imbalance in the peak compute-to-memory-capacity ratio for future servers. At the same time, the fraction contributed by memory systems to total datacenter costs and power consumption during typical usage is increasing. In response to these trends, this paper re-examines traditional compute-memory co-location on a single system and details the design of a new general-purpose architectural building block-a memory blade-that allows memory to be "disaggregated" across a system ensemble. This remote memory blade can be used for memory capacity expansion to improve performance and for sharing memory across servers to reduce provisioning and power costs. We use this memory blade building block to propose two new system architecture solutions-(1) page-swapped remote memory at the virtualization layer, and (2) block-access remote memory with support in the coherence hardware-that enable transparent memory expansion and sharing on commodity-based systems. Using simulations of a mix of enterprise benchmarks supplemented with traces from live datacenters, we demonstrate that memory disaggregation can provide substantial performance benefits (on average 10X) in memory constrained environments, while the sharing enabled by our solutions can improve performance-per-dollar by up to 57% when optimizing memory provisioning across multiple servers.},
booktitle = {Proceedings of the 36th Annual International Symposium on Computer Architecture},
pages = {267–278},
numpages = {12},
keywords = {memory capacity expansion, power and cost efficiencies, disaggregated memory, memory blades},
location = {Austin, TX, USA},
series = {ISCA '09}
}

@inproceedings{sherman,
  author = {Wang, Qing and Lu, Youyou and Shu, Jiwu},
  title = "Sherman: A Write-Optimized Distributed {B+Tree} Index on Disaggregated Memory",
  year = {2022},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3514221.3517824},
  doi = {10.1145/3514221.3517824},
  booktitle = {Proceedings of the 2022 International Conference on Management of Data},
  pages = {1033–1048},
  numpages = {16}, keywords = {index, disaggregated memory, RDMA},
  address = {Philadelphia, PA},
  }

%misc{sherman,
%  doi = {10.48550/ARXIV.2112.07320},
%  url = {https://arxiv.org/abs/2112.07320},
%  author = {Wang, Qing and Lu, Youyou and Shu, Jiwu},
%  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), Databases (cs.DB), Networking and Internet Architecture (cs.NI), FOS: Computer and information sciences, FOS: Computer and information sciences},
%  title = {Sherman: A Write-Optimized Distributed B+Tree Index on Disaggregated Memory},
%  publisher = {arXiv},
%  year = {2021},
%  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
%}

@inproceedings {race,
author = {Pengfei Zuo and Jiazhao Sun and Liu Yang and Shuangwu Zhang and Yu Hua},
title = {One-sided {RDMA-Conscious} Extendible Hashing for Disaggregated Memory},
booktitle = {2021 USENIX Annual Technical Conference (USENIX ATC 21)},
year = {2021},
isbn = {978-1-939133-23-6},
pages = {15--29},
url = {https://www.usenix.org/conference/atc21/presentation/zuo},
publisher = {USENIX Association},
month = jul,
}

@inproceedings {direct-cxl,
author = {Donghyun Gouk and Sangwon Lee and Miryeong Kwon and Myoungsoo Jung},
title = {Direct Access, {High-Performance} Memory Disaggregation with {DirectCXL}},
booktitle = {2022 USENIX Annual Technical Conference (USENIX ATC 22)},
year = {2022},
isbn = {978-1-939133-29-65},
address = {Carlsbad, CA},
pages = {287--294},
url = {https://www.usenix.org/conference/atc22/presentation/gouk},
publisher = {USENIX Association},
month = jul,
}

@misc{microsoft-cxl-first-gen,
  doi = {10.48550/ARXIV.2203.00241},
  url = {https://arxiv.org/abs/2203.00241},
  author = {Li, Huaicheng and Berger, Daniel S. and Novakovic, Stanko and Hsu, Lisa and Ernst, Dan and Zardoshti, Pantea and Shah, Monish and Agarwal, Ishwar and Hill, Mark D. and Fontoura, Marcus and Bianchini, Ricardo},
  keywords = {Operating Systems (cs.OS), Performance (cs.PF), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {First-generation Memory Disaggregation for Cloud Platforms},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@misc{facebook-cxl-tpp,
  doi = {10.48550/ARXIV.2206.02878},
  url = {https://arxiv.org/abs/2206.02878},

pages = {785--808},
url = {https://www.usenix.org/conference/nsdi21/presentation/sapio},
publisher = {USENIX Association},
month = apr,
}

@inproceedings{netlock,
author = {Yu, Zhuolong and Zhang, Yiwen and Braverman, Vladimir and Chowdhury, Mosharaf and Jin, Xin},
title = {NetLock: Fast, Centralized Lock Management Using Programmable Switches},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405857},
doi = {10.1145/3387514.3405857},
abstract = {Lock managers are widely used by distributed systems. Traditional centralized lock managers can easily support policies between multiple users using global knowledge, but they suffer from low performance. In contrast, emerging decentralized approaches are faster but cannot provide flexible policy support. Furthermore, performance in both cases is limited by the server capability.We present NetLock, a new centralized lock manager that co-designs servers and network switches to achieve high performance without sacrificing flexibility in policy support. The key idea of NetLock is to exploit the capability of emerging programmable switches to directly process lock requests in the switch data plane. Due to the limited switch memory, we design a memory management mechanism to seamlessly integrate the switch and server memory. To realize the locking functionality in the switch, we design a custom data plane module that efficiently pools multiple register arrays together to maximize memory utilization We have implemented a NetLock prototype with a Barefoot Tofino switch and a cluster of commodity servers. Evaluation results show that NetLock improves the throughput by 14.0-18.4x, and reduces the average and 99% latency by 4.7-20.3x and 10.4-18.7x over DSLR, a state-of-the-art RDMA-based solution, while providing flexible policy support.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {126–138},
numpages = {13},
keywords = {Data plane, Lock Management, Programmable Switches, Centralized},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings{p4-telem,
  author={Beltman, Rutger and Knossen, Silke and Hill, Joseph and Grosso, Paola},
  booktitle={2020 IEEE/ACM Innovating the Network for Data-Intensive Science (INDIS)}, 
  title={Using P4 and RDMA to collect telemetry data}, 
  year={2020},
  volume={},
  number={},
  pages={1-9},
  doi={10.1109/INDIS51933.2020.00006}}

 

@inproceedings{netcache,
author = {Jin, Xin and Li, Xiaozhou and Zhang, Haoyu and Soul\'{e}, Robert and Lee, Jeongkeun and Foster, Nate and Kim, Changhoon and Stoica, Ion},
title = {NetCache: Balancing Key-Value Stores with Fast In-Network Caching},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132764},
doi = {10.1145/3132747.3132764},
abstract = {We present NetCache, a new key-value store architecture that leverages the power and flexibility of new-generation programmable switches to handle queries on hot items and balance the load across storage nodes. NetCache provides high aggregate throughput and low latency even under highly-skewed and rapidly-changing workloads. The core of NetCache is a packet-processing pipeline that exploits the capabilities of modern programmable switch ASICs to efficiently detect, index, cache and serve hot key-value items in the switch data plane. Additionally, our solution guarantees cache coherence with minimal overhead. We implement a NetCache prototype on Barefoot Tofino switches and commodity servers and demonstrate that a single switch can process 2+ billion queries per second for 64K items with 16-byte keys and 128-byte values, while only consuming a small portion of its hardware resources. To the best of our knowledge, this is the first time that a sophisticated application-level functionality, such as in-network caching, has been shown to run at line rate on programmable switches. Furthermore, we show that NetCache improves the throughput by 3-10x and reduces the latency of up to 40% of queries by 50%, for high-performance, in-memory key-value stores.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {121–136},
numpages = {16},
keywords = {Programmable switches, Caching, Key-value stores},
location = {Shanghai, China},
series = {SOSP '17}
}

@inproceedings{tea,
author = {Kim, Daehyeok and Liu, Zaoxing and Zhu, Yibo and Kim, Changhoon and Lee, Jeongkeun and Sekar, Vyas and Seshan, Srinivasan},
title = {TEA: Enabling State-Intensive Network Functions on Programmable Switches},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405855},
doi = {10.1145/3387514.3405855},
abstract = {Programmable switches have been touted as an attractive alternative for deploying network functions (NFs) such as network address translators (NATs), load balancers, and firewalls. However, their limited memory capacity has been a major stumbling block that has stymied their adoption for supporting state-intensive NFs such as cloud-scale NATs and load balancers that maintain millions of flow-table entries. In this paper, we explore a new approach that leverages DRAM on servers available in typical NFV clusters. Our new system architecture, called TEA (Table Extension Architecture), provides a virtual table abstraction that allows NFs on programmable switches to look up large virtual tables built on external DRAM. Our approach enables switch ASICs to access external DRAM purely in the data plane without involving CPUs on servers. We address key design and implementation challenges in realizing this idea. We demonstrate its feasibility and practicality with our implementation on a Tofino-based programmable switch. Our evaluation shows that NFs built with TEA can look up table entries on external DRAM with low and predictable latency (1.8-2.2 μs) and the lookup throughput can be linearly scaled with additional servers (138 million lookups per seconds with 8 servers).},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {90–106},
numpages = {17},
keywords = {Programmable switches, Remote Direct Memory Access, Network Function Virtualization, Programmable networks, Data centers},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings{eris,
author = {Li, Jialin and Michael, Ellis and Ports, Dan R. K.},
title = {Eris: Coordination-Free Consistent Transactions using Network Multi-Sequencing},
booktitle = {Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP '17)},
year = {2017},
month = {October},
abstract = {Distributed storage systems aim to provide strong consistency and isolation guarantees on an architecture that is partitioned across multiple shards for scalability and replicated for fault-tolerance. Traditionally, achieving all of these goals has required an expensive combination of atomic commitment and replication protocols -- introducing extensive coordination overhead. Our system, Eris, takes a very different approach. It moves a core piece of concurrency control functionality, which we term multi-sequencing, into the datacenter network itself. This network primitive takes on the responsibility for consistently ordering transactions, and a new lightweight transaction protocol ensures atomicity.

The end result is that Eris avoids both replication and transaction coordination overhead: we show that it can process a large class of distributed transactions in a single round-trip from the client to the storage system without any explicit coordination between shards or replicas. It provides atomicity, consistency, and fault-tolerance with less than 10% overhead -- achieving throughput 4.5--35x higher and latency 72--80% lower than a conventional design on standard benchmarks.},
url = {https://www.microsoft.com/en-us/research/publication/eris-coordination-free-consistent-transactions-using-network-multi-sequencing/},
edition = {Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP '17)},
}

@inproceedings {no,
author = {Jialin Li and Ellis Michael and Naveen Kr. Sharma and Adriana Szekeres and Dan R. K. Ports},
title = {Just Say {NO} to Paxos Overhead: Replacing Consensus with Network Ordering},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {467--483},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/li},
publisher = {USENIX Association},
month = nov,
}

@inproceedings{when-computer,
author = {Ports, Dan R. K. and Nelson, Jacob},
title = {When Should The Network Be The Computer?},
year = {2019},
isbn = {9781450367271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3317550.3321439},
doi = {10.1145/3317550.3321439},
abstract = {Researchers have repurposed programmable network devices to place small amounts of application computation in the network, sometimes yielding orders-of-magnitude performance gains. At the same time, effectively using these devices requires careful use of limited resources and managing deployment challenges.This paper provides a framework for principled use of in-network processing. We provide a set of guidelines for building robust and deployable in-network primives, along with a taxonomy to help identify which applications can benefit from in-network processing and what types of devices they should use.},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages = {209–215},
numpages = {7},
keywords = {smart NICs, in-network computation, reconfigurable devices, programmable switches},
location = {Bertinoro, Italy},
series = {HotOS '19}
}

@inproceedings{rma,
author = {Singhvi, Arjun and Akella, Aditya and Gibson, Dan and Wenisch, Thomas F. and Wong-Chan, Monica and Clark, Sean and Martin, Milo M. K. and McLaren, Moray and Chandra, Prashant and Cauble, Rob and Wassel, Hassan M. G. and Montazeri, Behnam and Sabato, Simon L. and Scherpelz, Joel and Vahdat, Amin},
title = {1RMA: Re-Envisioning Remote Memory Access for Multi-Tenant Datacenters},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405897},
doi = {10.1145/3387514.3405897},
abstract = {Remote Direct Memory Access (RDMA) plays a key role in supporting performance-hungry datacenter applications. However, existing RDMA technologies are ill-suited to multi-tenant datacenters, where applications run at massive scales, tenants require isolation and security, and the workload mix changes over time. Our experiences seeking to operationalize RDMA at scale indicate that these ills are rooted in standard RDMA's basic design attributes: connectionorientedness and complex policies baked into hardware.We describe a new approach to remote memory access -- One-Shot RMA (1RMA) -- suited to the constraints imposed by our multi-tenant datacenter settings. The 1RMA NIC is connection-free and fixed-function; it treats each RMA operation independently, assisting software by offering fine-grained delay measurements and fast failure notifications. 1RMA software provides operation pacing, congestion control, failure recovery, and inter-operation ordering, when needed. The NIC, deployed in our production datacenters, supports encryption at line rate (100Gbps and 100M ops/sec) with minimal performance/availability disruption for encryption key rotation.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {708–721},
numpages = {14},
keywords = {Connection Free, Remote Memory Access, Congestion Control},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings{flock,
author = {Monga, Sumit Kumar and Kashyap, Sanidhya and Min, Changwoo},
title = {Birds of a Feather Flock Together: Scaling RDMA RPCs with Flock},
year = {2021},
isbn = {9781450387095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477132.3483576},
doi = {10.1145/3477132.3483576},
abstract = {RDMA-capable networks are gaining traction with datacenter deployments due to their high throughput, low latency, CPU efficiency, and advanced features, such as remote memory operations. However, efficiently utilizing RDMA capability in a common setting of high fan-in, fan-out asymmetric network topology is challenging. For instance, using RDMA programming features comes at the cost of connection scalability, which does not scale with increasing cluster size. To address that, several works forgo some RDMA features by only focusing on conventional RPC APIs.In this work, we strive to exploit the full capability of RDMA, while scaling the number of connections regardless of the cluster size. We present Flock, a communication framework for RDMA networks that uses hardware provided reliable connection. Using a partially shared model, Flock departs from the conventional RDMA design by enabling connection sharing among threads, which provides significant performance improvements contrary to the widely held belief that connection sharing deteriorates performance. At its core, Flock uses a connection handle abstraction for connection multiplexing; a new coalescing-based synchronization approach for efficient network utilization; and a load-control mechanism for connections with symbiotic send-recv scheduling, which reduces the synchronization overheads associated with connection sharing along with ensuring fair utilization of network connections. We demonstrate the benefits for a distributed transaction processing system and an in-memory index, where it outperforms other RPC systems by up to 88% and 50%, respectively, with significant reductions in median and tail latency.},
booktitle = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
pages = {212–227},
numpages = {16},
keywords = {Network hardware, Remote Memory Access},
location = {Virtual Event, Germany},
series = {SOSP '21}
}

@inproceedings{understanding-pcie,
author = {Neugebauer, Rolf and Antichi, Gianni and Zazo, Jos\'{e} Fernando and Audzevich, Yury and L\'{o}pez-Buedo, Sergio and Moore, Andrew W.},
title = {Understanding PCIe Performance for End Host Networking},
year = {2018},
isbn = {9781450355674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230543.3230560},
doi = {10.1145/3230543.3230560},
abstract = {In recent years, spurred on by the development and availability of programmable NICs, end hosts have increasingly become the enforcement point for core network functions such as load balancing, congestion control, and application specific network offloads. However, implementing custom designs on programmable NICs is not easy: many potential bottlenecks can impact performance.This paper focuses on the performance implication of PCIe, the de-facto I/O interconnect in contemporary servers, when interacting with the host architecture and device drivers. We present a theoretical model for PCIe and pcie-bench, an open-source suite, that allows developers to gain an accurate and deep understanding of the PCIe substrate. Using pcie-bench, we characterize the PCIe subsystem in modern servers. We highlight surprising differences in PCIe implementations, evaluate the undesirable impact of PCIe features such as IOMMUs, and show the practical limits for common network cards operating at 40Gb/s and beyond. Furthermore, through pcie-bench we gained insights which guided software and future hardware architectures for both commercial and research oriented network cards and DMA engines.},
booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
pages = {327–341},
numpages = {15},
keywords = {operating system, PCIe, reconfigurable hardware},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

@INPROCEEDINGS{star,
  author={Wang, Xizheng and Chen, Guo and Yin, Xijin and Dai, Huichen and Li, Bojie and Fu, Binzhang and Tan, Kun},
  booktitle={Proceedings of the 29th IEEE International Conference on Network Protocols (ICNP)}, 
  title="{StaR}: Breaking the Scalability Limit for {RDMA}", 
  year={2021},
  }

@inproceedings{scalerpc, author = {Chen, Youmin and Lu, Youyou and Shu, Jiwu}, title = {Scalable RDMA RPC on Reliable Connection with Efficient Resource Sharing}, year = {2019}, isbn = {9781450362818}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3302424.3303968}, doi = {10.1145/3302424.3303968}, booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019}, articleno = {19}, numpages = {14}, keywords = {RDMA, Scalability, Resource Sharing}, location = {Dresden, Germany}, series = {EuroSys '19} }

@inproceedings {filemr,
author = {Jian Yang and Joseph Izraelevitz and Steven Swanson},
title = {{FileMR}: Rethinking {RDMA} Networking for Scalable Persistent Memory },
booktitle = {Proceedings of the 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI)},
year = {2020},
pages = {111--125},
month = feb,
}

@inproceedings{snap,
title	= {Snap: a Microkernel Approach to Host Networking},
author	= {Michael Marty and Marc de Kruijf and Jacob Adriaens and Christopher Alfeld and Sean Bauer and Carlo Contavalli and Mike Dalton and Nandita Dukkipati and William C. Evans and Steve Gribble and Nicholas Kidd and Roman Kononov and Gautam Kumar and Carl Mauer and Emily Musick and Lena Olson and Mike Ryan and Erik Rubow and Kevin Springborn and Paul Turner and Valas Valancius and Xi Wang and Amin Vahdat},
year	= {2019},
booktitle	= {In ACM SIGOPS 27th Symposium on Operating Systems Principles},
address	= {New York, NY, USA}
}

@inproceedings{prism,
author = {Dharanipragada, Sowmya and Joyner, Shannon and Burke, Matthew and Szekeres, Adriana and Nelson, Jacob and Zhang, Irene and Ports, Dan R. K.},
title = {PRISM: Rethinking the RDMA Interface for Distributed Systems},
booktitle = {SOSP 2021},
year = {2021},
month = {October},
abstract = {Remote Direct Memory Access (RDMA) has been used to accelerate a variety of distributed systems, by providing low-latency, CPU-bypassing access to a remote host's memory. However, most of the distributed protocols used in these systems cannot easily be expressed in terms of the simple memory READs and WRITEs provided by RDMA. As a result, designers face a choice between introducing additional protocol complexity (e.g., additional round trips) or forgoing the benefits of RDMA entirely. This paper argues that an extension to the RDMA interface can resolve this dilemma. We introduce the PRISM interface, which extends the RDMA interface with four new primitives: indirection, allocation, enhanced compare-and-swap, and operation chaining. These increase the expressivity of the RDMA interface, while still being implementable using the same underlying hardware features. We show their utility by designing three new applications using PRISM primitives, that require little to no server-side CPU involvement: (1) PRISM-KV, a key-value store; (2) PRISM-RS a replicated block store; and (3) PRISM-TX, a distributed transaction protocol. Using a software-based implementation of the PRISM primitives, we show that these systems outperform prior RDMA-based equivalents.},
url = {https://www.microsoft.com/en-us/research/publication/prism-rethinking-the-rdma-interface-for-distributed-systems/},
}

@inproceedings{mind,
author = {Lee, Seung-seob and Yu, Yanpeng and Tang, Yupeng and Khandelwal, Anurag and Zhong, Lin and Bhattacharjee, Abhishek},
title = {MIND: In-Network Memory Management for Disaggregated Data Centers},
year = {2021},
isbn = {9781450387095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477132.3483561},
doi = {10.1145/3477132.3483561},
abstract = {Memory disaggregation promises transparent elasticity, high resource utilization and hardware heterogeneity in data centers by physically separating memory and compute into network-attached resource "blades". However, existing designs achieve performance at the cost of resource elasticity, restricting memory sharing to a single compute blade to avoid costly memory coherence traffic over the network.In this work, we show that emerging programmable network switches can enable an efficient shared memory abstraction for disaggregated architectures by placing memory management logic in the network fabric. We find that centralizing memory management in the network permits bandwidth and latency-efficient realization of in-network cache coherence protocols, while programmable switch ASICs support other memory management logic at line-rate. We realize these insights into MIND1, an in-network memory management unit for rack-scale disaggregation. MIND enables transparent resource elasticity while matching the performance of prior memory disaggregation proposals for real-world workloads.},
booktitle = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
pages = {488–504},
numpages = {17},
keywords = {Memory disaggregation, Programmable networks},
location = {Virtual Event, Germany},
series = {SOSP '21}
}

@inproceedings{kona,
author = {Calciu, Irina and Imran, M. Talha and Puddu, Ivan and Kashyap, Sanidhya and Maruf, Hasan Al and Mutlu, Onur and Kolli, Aasheesh},
title = {Rethinking Software Runtimes for Disaggregated Memory},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446713},
doi = {10.1145/3445814.3446713},
abstract = {Disaggregated memory can address resource provisioning inefficiencies in current datacenters. Multiple software runtimes for disaggregated memory have been proposed in an attempt to make disaggregated memory practical. These systems rely on the virtual memory subsystem to transparently offer disaggregated memory to applications using a local memory abstraction. Unfortunately, using virtual memory for disaggregation has multiple limitations, including high overhead that comes from the use of page faults to identify what data to fetch and cache locally, and high dirty data amplification that comes from the use of page-granularity for tracking changes to the cached data (4KB or higher). In this paper, we propose a fundamentally new approach to designing software runtimes for disaggregated memory that addresses these limitations. Our main observation is that we can use cache coherence instead of virtual memory for tracking applications' memory accesses transparently, at cache-line granularity. This simple idea (1) eliminates page faults from the application critical path when accessing remote data, and (2) decouples the application memory access tracking from the virtual memory page size, enabling cache-line granularity dirty data tracking and eviction. Using this observation, we implemented a new software runtime for disaggregated memory that improves average memory access time by 1.7-5X and reduces dirty data amplification by 2-10X, compared to state-of-the-art systems.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {79–92},
numpages = {14},
keywords = {remote memory, disaggregated memory, cache coherence},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@inproceedings {faast,
author = {Anuj Kalia and Michael Kaminsky and David G. Andersen},
title = {{FaSST}: Fast, Scalable and Simple Distributed Transactions with {Two-Sided} ({{{{{RDMA}}}}}) Datagram {RPCs}},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {185--201},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/kalia},
publisher = {USENIX Association},
month = nov,
}

@inproceedings {netchain,
author = {Xin Jin and Xiaozhou Li and Haoyu Zhang and Nate Foster and Jeongkeun Lee and Robert Soul{\'e} and Changhoon Kim and Ion Stoica},
title = {{NetChain}: {Scale-Free} {Sub-RTT} Coordination},
booktitle = {15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18)},
year = {2018},
isbn = {978-1-939133-01-4},
address = {Renton, WA},
pages = {35--49},
url = {https://www.usenix.org/conference/nsdi18/presentation/jin},
publisher = {USENIX Association},
month = apr,
}

@inproceedings{netkv,
  author={Zhang, Wei and Wood, Timothy and Hwang, Jinho},
  booktitle={2016 IEEE International Conference on Autonomic Computing (ICAC)}, 
  title={NetKV: Scalable, Self-Managing, Load Balancing as a Network Function}, 
  year={2016},
  volume={},
  number={},
  pages={5-14},
  doi={10.1109/ICAC.2016.28}}

@inproceedings {bedrock,
author = {Jiarong Xing and Kuo-Feng Hsu and Yiming Qiu and Ziyang Yang and Hongyi Liu and Ang Chen},
title = {Bedrock: Programmable Network Support for Secure {RDMA} Systems},
booktitle = {31st USENIX Security Symposium (USENIX Security 22)},
year = {2022},
isbn = {978-1-939133-31-1},
address = {Boston, MA},
pages = {2585--2600},
url = {https://www.usenix.org/conference/usenixsecurity22/presentation/xing},
publisher = {USENIX Association},
month = aug,
}

@article{p4telemetry,
  title={Using P4 and RDMA to collect telemetry data},
  author={Rutger Beltman and Silke Knossen and Joseph Hill and Paola Grosso},
  journal={2020 IEEE/ACM Innovating the Network for Data-Intensive Science (INDIS)},
  year={2020},
  pages={1-9}
}

@inproceedings {switchml,
author = {Amedeo Sapio and Marco Canini and Chen-Yu Ho and Jacob Nelson and Panos Kalnis and Changhoon Kim and Arvind Krishnamurthy and Masoud Moshref and Dan Ports and Peter Richtarik},
title = {Scaling Distributed Machine Learning with {In-Network} Aggregation},
booktitle = {18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21)},
year = {2021},
isbn = {978-1-939133-21-2},
pages = {785--808},
url = {https://www.usenix.org/conference/nsdi21/presentation/sapio},
publisher = {USENIX Association},
month = apr,
}