\section{Background}

\subsection{Disaggregated Memory}

Memory disaggregation separates primary storage from the CPU
by a fast network. The question \textit{how to present
memory over the network to an application?} is still open,
and a spectrum of system designs currently exist.
Transparent systems present far memory to the application as
if it were local. Virtual memory enables paging systems to
keep a cache of local pages, and swap in and out
transparently on page faults ~\cite{infiniswap, leap,
fastswap}.  Some have argued that page granularity is too
course and built upon an incoherent caching
layer~\cite{kona, legoos}.  In both cases transparency
incurs a performance cost as remote accesses cannot be
optimized for by the running application.
%%
Alternativly applications can access remote memory
explicitly with an API similar to an RPC
call~\cite{aifm,reigons,clover,sherman,faast}. Explicit
access allows for optimized remote requests. They can be
batched, scheduled, and managed by libraries and runtimes. 
%%
Whether explicit or transparent sharing is expensive, at the
time of writing no transparent system supports shared memory
-- the cost of cache coherence is prohibitively high in
terms of bandwidth and latency. Explicit cases are more
promising for sharing as they allow application developers
to acquire locks, and for libraries to make use of
optimistically concurrent datastrcutrues.

\subsection{RDMA protocol}


%% RDMA
Remote direct memory access (RDMA) is a network protocol
which allows NICs to bypass CPUs and access host memory
directly.  The RDMA protocol consists of a set of verbs
which abstract remote memory instructions. Instruction
execution and connection state are entirely managed by the
NIC which exposes the verbs API to the CPU. The CPU
registers memory regions for DMA with the NIC and sets up
connections (Queue Pairs) with a remote RDMA enabled NIC.
%%
RDMA connections come in a variety of flavors, each of which
enables a different set of RDMA verbs and delivary
guarantee~\cite{herd, erpc, storm}. Unreliable Datagram
(UD), and Unreliable Connection (UC) operate similar to UDP
with no reliable delivery or ordering guarantees and a
restricted set of verbs.  Reliable Connected (RC) operates
similar to TCP, the NIC manages connection states for each
QP and ensures reliable in-order delivery by using sequence
numbers and a go-back-n retransmission protocol.
%%
Serious debate exists over which connections to
use~\cite{storm,cell,herd,faast,farm}, each has advantages
and disadvantages in terms of NIC resource utilization,
throughput, and latency. Disaggregated architectures have no
remote CPU's, in this proposed setting RC is the most
attractive as it alone enables the use of one-sided verbs
:\textit{Read}, \textit{Write}, and the atomic \textit{CAS}
.
%%
RDMA atomic instructions provide serialization without the
need for a remote CPU. When an atomic instruction is
executed the memory side NIC examines all in-flight
operations across each QP and ensures that the atomic
operation executes transitionally. Atomics are expensive as
they require buffering on QP's and a PCIe round trip before
the NIC can respond to the request.  Atomic requests are
high latency compared to reads and writes (approximately 2x
in the best case). As a performance mitigation Mellanox NICs
supply a small (few KB) region of device mapped memory which
allows the memory side NIC to execute RDMA instructions on
it's local device memory without incurring a PCIe round
trip. Executing atomics on this memory is ~\todo{5x} higher
throughput than host memory, but still slower than it's read
and write counterparts. Some projects have made use of this
memory region to accelerate remote memory
locks~\cite{sherman}


%% RDMA Connections
%% RDMA Atomics

\subsection{Programmable Switches} Most proposals for
disaggregation are at rack-scale. They propose a single rack
with servers partitioned into roles: compute, memory, and
storage, each of which is interconnected by a TOR.  The TOR
is central in this architecture, a fact which has not gone
unnoticed by system designers who argue that a programmable
switch can facilitate remote memory apis, and OS
functionality~\cite{disandapp,mind}. 
%%
Literature on offloading OS, and service level functionality
to programmable switches is
plentiful~\cite{netlock,netkv,netchain,netcache}. The
constraints in each case are similar, switches have limited
memory, and processing capabilities, if the computational
ask of the switch is too high packets must be recirculated
adding additional latency, and reducing aggregate bandwidth.
Ideal applications for programmable switches use little
memory, require little processing and enable a huge
performance benefit from centralization, and the billions of
operations (in terms of packets) that a switch can process
per second.
%%
Prior work has shown that programmable switches are able to
manage locks~\cite{netlock}, track the state required to
maintain an RDMA reliable connection~\cite{tea}, and provide
rack scale serialization at low
cost~\cite{eris,no,when-computer}. These properties make a
top-of-rack programable switch ideal for managing remote
memory as it can guard access, maintain connections, and
provide serialization primitives for all clients.


\section{Serialization}

\begin{figure}[t]
  \includegraphics[width=0.485\textwidth]{fig/rdma_concur.pdf}
  \vskip -0.5em

    \caption{Achieved throughput of RDMA verbs across twenty queue
      pairs on data-independent addresses as a function of request
      concurrency.  When using atomic requests, ConnectX-5 NICs can
      support approximately 2.7 MOPS per queue pair, up to about 55
      MOPS in aggregate.}

    \label{fig:rdma_concur}
      \vskip -0.5em
\end{figure}

\begin{figure}[t]
    \includegraphics[width=0.485\textwidth]{fig/success_rate.pdf}
    \vskip -0.5em
    \caption{Percentage of successful operations in a
      50:50 read-write workload spread across 1,024 keys according
      to a Zipf(0.99) distribution as more client threads are
      added. At 240 threads less than 4\% of operations succeed.}
      
    \vskip -0.5em
    \label{fig:success_rate}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=0.485\textwidth]{fig/cas_vs_writes.pdf}
%  \vskip -0.5em

  \caption{ Throughput comparison of serialized RDMA operations in
    NIC-mapped device and main memory. Writes obtain 6.2$\times$ higher
    throughput than CAS in host memory and 2.5$\times$ higher in NIC memory despite being restricted to a single queue pair.  }

    \label{fig:cas_vs_writes}
%      \vskip -0.5em
\end{figure}

\subsection{RDMA serialization} RDMA atomics instructions
serialize remote memory operations without the need for a
memory side CPU. These instructions \textit{Fetch-and-Add}
and \textit{Compare-and-Swap} (CAS) execute on 64 bit width
words and are guaranteed to have atomic visibility across
all queue pairs. These operations are expensive. On a single
address atomic instructions can only execute a few million
operations per second~\ref{fig:cas_vs_writes}, across
independent addresses they have approximately half the
throughput of their non atomic
counterparts~\ref{fig:rdma_concur}. 
%%
Data structures built with RDMA atomics have hard
performance limits because the aforementioned constraints.
Locks located at a single address which use traditional
lock, unlock operations are limited to around 500k accesses
per second. This assumes perfectly coordinated requests,
under contention requests which fail to acquire or release a
lock still consume operation bandwidth.
%%
Under contention RDMA has poor support for traditional
locking. In contrast optimistic data structures with locks
scattered throughout, such as a linked list, are not rate
limited by this single address restriction.  However, they
are fundamentally limited by the fact that any atomics have
half the throughput of reads and writes. More critically,
under contention optimistic data structures have no liveness
guarantees under contention atomic operations fail
frequently (Figure~\ref{fig:success_rate}).  RDMA has no
support for resolving pointers (pointer chasing) and
retrying operations for such data structures(~\cite{rma}), a
simple operation usually executed by a memory side CPU. As
such RDMA clients are forced to resolve their failures
requests themselves often retrying many times.
%%
Atomic operations are not the only serialization mechanism
provided by RDMA. Reliable connections provide in order
delivery on individual queue pairs. This allows clients to
issue multiple requests in parallel and allows the NIC to
resolve reordering and dropped packets using go-back-n This
allows clients to issue multiple requests in parallel and
allows the NIC to resolve reordering and dropped packets
using go-back-n retransmission. When clients are collocated,
queue pairs can be shared by multiple cores through
techniques like flat combining~\cite{flock,sherman}. This
technique removes the need for RDMA atomics, as clients can
locally resolve their conflicts and then issue their
requests to remote memory at the full throughput of reads
and writes. Unfortunately this technique is not applicable
to distributed clients as they do not share access to the
same queue pair

\subsection{Programmable Switch Serialization}

Switches can cheaply serialize packets~\cite{when-computer}.
Programable switches with P4 pipelines can use this cheap
serialization, along with their ability to manage small
amounts of in network state, to serialize distributed
applications. Mind for instance provides a unified TLB and
cache for disaggregated applications~\cite{mind}. Packets
processed by a programmable switch are sequenced in order,
updates to the switches registers are atomic with respect to
the packets as each state of the pipeline is occupied by
exactly one packet at a time. A centralized switch can
therefore apply monotonic sequence numbers to a stream of
packets, or maintain a lock without the need for explicit
atomic operations.

Unfortunately this serialization is not sufficient for RDMA
memory operations out of the box. RDMA packets on two
reliable connections may be ordered on the switch, and then
subsequently reordered by the receiving side NIC, or by the
PCIe bus~\cite{understanding-pcie}.

In all cases switch memory and compute are highly
constrained. Any operations that exceed the processing
limit of the match action pipeline require recirculation
which consumes additional switch bandwidth. For example,
terminating connections with a switch is prohibitively
expensive as both the state of the entire connections, and
the logic for connection startup and teardown would consume
large quantities of the switches buffers. Any logic, or data
offloaded to a programable switch must therefore be minimal
in order to meet the compute and data restrictions of the
switches SRAM.

\section{Swordbox}

We present \sword a middlebox solution for sharing remote
memory. \sword provides acceleration for both lock based and
optimistically concurrent data structures. Locks are managed
in network, and the results of locking operations are
forward to memory. Lock accesses are accelerated by
replacing RDMA atomic operations with writes in flight.
Modified lock operations are multiplexed onto existing RDMA
connections, per lock, to ensure serialized access to the
lock. \sword also provides a mechanism to remove contention
from optimistic data structures by caching the metadata
required to detect and resolve conflicts. These techniques
are implemented in a DPDK prototype for the lock based
approach and fully realized in P4 for optimistic data
structures.

\subsection{Caching}

\subsection{Locks}

\subsection{Optimistic Concurrency}