\section{background}

\subsection{Disaggregated Memory}

Memory disaggregation separates primary storage from the CPU
by a fast network. The question \textit{how to present
memory over the network to an application?} is still open,
and a spectrum of system designs currently exist.
Transparent systems present far memory to the application as
if it were local. Virtual memory enables paging systems to
keep a cache of local pages, and swap in and out
transparently on page faults ~\cite{infiniswap, leap,
fastswap}.  Some have argued that page granularity is too
course and built upon an incoherent caching
layer~\cite{kona, legoos}.  In both cases transparency
incurs a performance cost as remote accesses cannot be
optimized for by the running application.
%%
Alternativly applications can access remote memory
explicitly with an API similar to an RPC
call~\cite{aifm,reigons,clover,sherman,faast}. Explicit
access allows for optimized remote requests. They can be
batched, scheduled, and managed by libraries and runtimes. 
%%
Whether explicit or transparent sharing is expensive, at the
time of writing no transparent system supports shared memory
-- the cost of cache coherence is prohibitively high in
terms of bandwidth and latency. Explicit cases are more
promising for sharing as they allow application developers
to acquire locks, and for libraries to make use of
optimistically concurrent datastrcutrues.

\subsection{RDMA protocol}


%% RDMA
Remote direct memory access (RDMA) is a network protocol
which allows NICs to bypass CPUs and access host memory
directly.  The RDMA protocol consists of a set of verbs
which abstract remote memory instructions. Instruction
execution and connection state are entirely managed by the
NIC which exposes the verbs API to the CPU. The CPU
registers memory regions for DMA with the NIC and sets up
connections (Queue Pairs) with a remote RDMA enabled NIC.
%%
RDMA connections come in a variety of flavors, each of which
enables a different set of RDMA verbs and delivary
guarantee~\cite{herd, erpc, storm}. Unreliable Datagram
(UD), and Unreliable Connection (UC) operate similar to UDP
with no reliable delivery or ordering guarantees and a
restricted set of verbs.  Reliable Connected (RC) operates
similar to TCP, the NIC manages connection states for each
QP and ensures reliable in-order delivery by using sequence
numbers and a go-back-n retransmission protocol.
%%
Serious debate exists over which connections to
use~\cite{storm,cell,herd,faast,farm}, each has advantages
and disadvantages in terms of NIC resource utilization,
throughput, and latency. Disaggregated architectures have no
remote CPU's, in this proposed setting RC is the most
attractive as it alone enables the use of one-sided verbs
:\textit{Read}, \textit{Write}, and the atomic \textit{CAS}
.
%%
RDMA atomic instructions provide serialization without the
need for a remote CPU. When an atomic instruction is
executed the memory side NIC examines all in-flight
operations across each QP and ensures that the atomic
operation executes transitionally. Atomics are expensive as
they require buffering on QP's and a PCIe round trip before
the NIC can respond to the request.  Atomic requests are
high latency compared to reads and writes (approximately 2x
in the best case). As a performance mitigation Mellanox NICs
supply a small (few KB) region of device mapped memory which
allows the memory side NIC to execute RDMA instructions on
it's local device memory without incurring a PCIe round
trip. Executing atomics on this memory is ~\todo{5x} higher
throughput than host memory, but still slower than it's read
and write counterparts. Some projects have made use of this
memory region to accelerate remote memory
locks~\cite{sherman}


%% RDMA Connections
%% RDMA Atomics


\subsection{Programmable Switches}
Many disaggregated memory proposals are rack-scale. They
propose a single rack with servers of CPUs, banks of memory,
and SSD's, each of which is interconnected by a TOR. This
central role of the switch has not gone unnoticed and
disaggregated architects have proposed using modern
programmable switches to enable remote memory apis, and OS
functionality~\cite{disandapp,mind}. Literature on
offloading OS, and service level functionality to
programmable switches is plentiful Literature on offloading
OS, and service level functionality to programmable switches
is plentiful~\cite{netlock,netkv,netchain,netcache}, the
constraints in each case are similar, switches have limited
memory, and processing capabilities, if the computational
ask of the switch is too high packets must be recirculated
adding additional latency, and reducing aggregate bandwidth.
Ideal applications for programmable switches use little
memory, require little processing and enable a huge
performance benefit from centralization, and the billions of
operations (in terms of packets) that a switch can process
per second.

Prior work has shown that programmable switches are able to
manage locks~\cite{netlock}, track the state required to
maintain an RDMA reliable connection~\cite{tea}, and provide
rack scale serialization at low
cost~\cite{eris,no,when-computer}. These properties make a
top-of-rack programable switch ideal for managing remote
memory as it can guard access, maintain connections, and
provide serialization primitives for all clients.


\section{Serialization}

\subsection{RDMA serialization}
RDMA provides a set of atomic instructions for serializing
access to remote memory without the need for a memory side
CPU. These instructions \textit{Fetch-and-Add} and
\textit{Compare-and-Swap} (CAS) execute on 64 bit width
words and are guaranteed to be consistent across queue pairs
with atomic visibility to all observers. These operations
are expensive. On a single address atomic instructions can
only execute a few million operations per
second~\todo{Figure 1}, across independent addresses they
have approximately half the throughput of their non atomic
counterparts. These limitations fundamentally limit data
structures built with them, locks with a single address
requiring both a lock and unlock call to guard access are
limited to around 500k accesses per second. This assumes
perfectly coordinated access, under contention even failed
requests, attempting to aquire a taken lock with CAS, eats
into the operation bandwidth. Under contention RDMA has poor
support for traditional locking. In contrast optimistic data
structures with locks scattered throughout, such as a linked
list, are not rated limited by this single address
restriction. However, they are fundementally limited by the
fact that any atomics have half the throughput of reads and
writes. More critically, under contention optimistic data
structures have no liveness guarantees under contention
atomic operations fail frequently~\todo{Figure: failure to
succeed}. RDMA has no support for resolving pointers
(pointer chaising) and retrying operations for such data
strucutres(~\cite{rma}), a simple operation usually executed
by a memory side CPU. As such RDMA clients are forced to
resolve their failures requests themselves often retrying
many times.

Atomic operations are not the only serialization mechanism
provided by RDMA. Reliable connections provide in order
delivery on individual queue pairs. This allows clients to
issue multiple requests in parallel and allows the NIC to
resolve reordering and dropped packets using go-back-n This
allows clients to issue multiple requests in parallel and
allows the NIC to resolve reordering and dropped packets
using go-back-n retransmission. When clients are collocated,
queue pairs can be shared by multiple cores through
techniques like flat combining~\cite{flock,sherman}. This
technique removes the need for RDMA atomics, as clients can
locally resolve their conflicts and then issue their
requests to remote memory at the full throughput of reads
and writes. Unfortunately this technique is not applicable
to distributed clients as they do not share access to the
same queue pair

\subsection{Programmable Switch Serialization}

Switches can cheaply serialize packets~\cite{when-computer}.
Programable switches with P4 pipelines can use this cheap
serialization, along with their ability to manage small
amounts of in network state, to serialize distributed
applications. Mind for instance provides a unified TLB and
cache for disaggregated applications~\cite{mind}. Packets
processed by a programmable switch are sequenced in order,
updates to the switches registers are atomic with respect to
the packets as each state of the pipeline is occupied by
exactly one packet at a time. A centralized switch can
therefore apply monotonic sequence numbers to a stream of
packets, or maintain a lock without the need for explicit
atomic operations.

Unfortunately this serialization is not sufficient for RDMA
memory operations out of the box. RDMA packets on two
reliable connections may be ordered on the switch, and then
subsequently reordered by the receiving side NIC, or by the
PCIe bus~\cite{understanding-pcie}.

In all cases switch memory and compute are highly
constrained. Any operations that exceed the processing
limit of the match action pipeline require recirculation
which consumes additional switch bandwidth. For example,
terminating connections with a switch is prohibitively
expensive as both the state of the entire connections, and
the logic for connection startup and teardown would consume
large quantities of the switches buffers. Any logic, or data
offloaded to a programable switch must therefore be minimal
in order to meet the compute and data restrictions of the
switches SRAM.

\section{Swordbox}

We present \sword a middlebox solution for sharing remote
memory. \sword provides acceleration for both lock based and
optimistically concurrent data structures. Locks are managed
in network, and the results of locking operations are
forward to memory. Lock accesses are accelerated by
replacing RDMA atomic operations with writes in flight.
Modified lock operations are multiplexed onto existing RDMA
connections, per lock, to ensure serialized access to the
lock. \sword also provides a mechanism to remove contention
from optimistic data structures by caching the metadata
required to detect and resolve conflicts. These techniques
are implemented in a DPDK prototype for the lock based
approach and fully realized in P4 for optimistic data
structures.

\subsection{Caching}

\subsection{Locks}

\subsection{Optimistic Concurrency}