\section{Background}


\subsection{Resource Disaggregation}

Resource disaggregation is an architectural paradigm which separates disk, cpu
and memory over a network~\cite{requirements,legoos}. The goal of this
architecture is to enable extreme flexibility in terms of machine composition.
For example a systems memory capacity can be dynamically apportioned by
reconfiguration, rather than by manually changing the physical components of a
single machine~\cite{fastswap}. It is now common for disks (HHD and SSD) to be
disaggregated from CPU and memory. SSDs are comparatively easier to
disaggregated than main memory as their access latencies are on the order of
10's of microseconds which amortizes the network round trip cost~\cite{decible}.

Local memory latency is around 50ns. The cost of accessing memory over the
network is on the order of 1us -- approximately a 20x overhead. This order of
magnitude difference in latency makes hiding remote memory accesses a hard
problem.  

Within the disaggregated memory community there is a divide between using
software or hardware to access remote memory.  The use of custom hardware to
access and control memory over the network is referred to as memory
disaggregation. Here custom hardware is used to interpose on either the caching
or paging system. Alternatively the use of commodity hardware to access and
control remote memory is referred to as \textit{far memory} ~\cite{reigons,
legoos, clover}~\todo{more}. In this work we concentrate on the latter, our
solutions for accessesing and controlling remote memory are confined to existing
hardware, mainly RDMA capable NICs and programmable switches.

Choosing how to expose remote memory to users remains an open problem. Full
transparency implies that existing applications can make use of remote memory
without modification. Many systems leverage virtual memory and use remote memory
as a swap device fetching and evicting pages to and from remote memory.
~\cite{fastswap,GMS,infiniswap,leap}. Transparency costs performance systems
which make the remote accesses explicit typically have far higher performance
for similar operations~\cite{aifm}\todo{Find another good citation for this}.
Prior work on transparent remote memory does not consider the problem of sharing
remote memory. Transparency for resources such as shared locks has disastrous
performance penalties.  We take the latter approach and suggest that to achieve
the highest performance programs for remote memory should be built by engineers
who understand the constraints of remote memory, and expose common API's to
users, such as POSIX, PUT/GET, or language integrated runtimes.

\subsection{RDMA Key Value Stores}

RDMA has been used in a litany of work to build fast in memory key values
stores~\cite{MemC3,herd,pilaf,sonuma,storm}. These works provide deep insights
into the tradeoffs between RDMA verbs, and the spectrum of remote memory
architectures. In each it is assumed that a remote CPU is co-resident with
memory to provide some degree of serialization for writes and metadata
manipulation. In the context of disaggregation no such co-resident CPU exists.
In it's absence clients must serialize their own writes using RDMA atomic
operations. These operations are expensive and are known to be suboptimal in
comparison to fully asynchronous verbs~\cite{design-guidelines}.

\subsection{Programable Middleboxes}

Current proposals for disaggregated architectures are scoped at rack
scale~\cite{disandapp,beyond,firebox}.  The machinery for arbitrating memory
access varies between proposals but they each rely on a centralized controller.
Some are as simple as a scaled out PCIe root complex, while others imagine a
programable middle box with an API exposed to applications~\cite{disandapp}. We
see the latter as a promising opportunity as it allows for developers to highly
optimize their programs for remote memory.  In this work we envision rack scale
computing with either a programmable switch, or some other programmable hardware
(FPGA) being used as a centralized switching fabric for memory
operations~\cite{supernic}. We assume that these middle boxes have highly
constrained resources such as a limited amount of SRAM intended for forwarding
packets with limited functionality for executing programs in the data path.

\subsection{Clover}

Clover is a key value store designed for disaggregated persistent
memory~\cite{clover}. While it targets persistent storage it is also a
prototypical example of a key value store for remote DRAM. It's design makes the
assumption that there are no remote CPU's coresident with memory. All of Clovers
remote memory accesses are made with one sided RDMA operations. Reads, Writes,
and CAS. Clover's design moves metadata storage off of the data path. In the
data path reads are writes are made to an append only linked list stored in
remote memory. All operations are made to the tail of the list. A client may not
know the location of the tail as other writers may concurrently push it forward.
When a read or a write fails to land on the tail of the list clover iteratively
traverses the structure until the tail is found. While this provides no liveness
guarantees in the common read heavy case concurrent clients all eventually reach
the end of the list. To speed up operations clients keep caches of a pointer to
the end of each keys linked list to avoid traversals. When writes are heavy, and
when single keys are hot, clovers performance degrades substantially. In order
to make writes serialized Clover uses RDMA CAS operations which when used
frequently on the same location lead to abysmal performance. In cases when CAS's
fail, clover performs a read for the tail and then retries the CAS. Therefore
this number inflates as the number of clients and conflicts increase without
guaranteeing fair forward progress.