\section{Implementation}

\begin{figure}
    \includegraphics[width=0.45\textwidth]{fig/packet_processing.pdf}
    \caption{Our packet processing pipeline}
    \label{fig:system}
\end{figure}

Our {\sword} prototype is implemented in a software switch using DPDK,
but is designed to have low memory and computational overhead making
it suitable for deployment on programmable switch hardware.  Although
our current evaluation is based on Clover, {\sword} is intended to be
general purpose, and comparmentalizes application-specific logic to
the extent possible.



\subsection{Traffic identification} Depending on the design of a disaggregated
rack memory traffic might be coresident with regular network traffic.
Additionally some of the traffic on the memory bus may not require tracking or
manipulation. In the case of clover we do not modify any traffic to the metadata
server as it is not in the read/write path. The first stage of our packet
processing pipeline is to match requests for manipulation. In our design users
submit a filter as part of their program to allow traffic which does not need to
be modified to flow freely.

\subsection{Dynamic enable/disable of connections, and epochs}

A key goal of our approach is to support the deployment of
serializer-based performance enhancements without modifying the far
memory system itself---including requiring any explicit negotiation
with the serializer.
%For instance is designed to deal with
%memory operations made to the wrong location via iterative pointer
%chasing. We strongly suggest that disaggregated algorithms take this
%approach as our middlebox solution only acts to acclerate operations
%in the common case.
We add and subtract connections based on the send
and receipt of a single CAS operation. The QP and sequence number for
the CAS are stored on send, and the ATOMIC ACK is used to retreive the
other recevers QP. As this approach requires only a single packet
requests can be added and removed from our algorithm dynamically with
little effort. As some state may be dependent on the number of
connections (such as the key to QP mappings), state transitions either
require a lock, or the coping of current state over to a new epoch
when new connections are added. In all of our experiments only one
such transition is made. We begin our mapping after a specific number
of clients for the experiment have connected. Once the total number of
clients have connected, a switch is flipped, and the QP multiplexing
algorithm begins. Requests which do not have mappings stored, but were
in flight during the flip have their sequence numbers, and MSN values
applied to the connection state of the new epoch.

%\subsection{Operation caching}
%\label{sec:operation-caching}



% \begin{figure}
%     \includegraphics[width=0.45\textwidth]{fig/throughput.pdf}
%     \caption{Default Clover throughput vs. Clover with write conflict
%     detection and correction turned on \todo{recompute with the read caching values (old)}}
%     \label{fig:throughput}
%     \vskip -1em
% \end{figure}

\subsection{Implementing Atomic replacement}

In the
following subsections we describe the dangers of removing atomics, and present
our solutions.

A few assumptions must be made in order for this replacement of operations to be
made. First and foremost all operation serialization must be made, and finalized
at the point where the CAS is swapped out. More formally, all of the data
structure invariants which required locking, must be satisfied at the time of
transforming the packet. Further the order of operations must be maintained
downstream from the checking of the invariant. These two requirements influence
the design of any system which aims to make this performance improvement.


%% ACS - This is just lifted from WORDS; no need to repeat here

%% \begin{figure}
%%     \includegraphics[width=0.45\textwidth]{fig/cache.pdf}
%%     \caption{Performance as a function of keys cached. Caching a few
%%     of the top-$N$ keys provides the greatest marginal throughput
%%     benefits.}
%%     \label{fig:cache}
%% \end{figure}

%% \textbf{reduced cache size} we show that if hot keys are known we require only a
%% small amount of in network state~\ref{fig:cache} we have considered dynamic
%% approaches such as LRU which would allows for a finite amount of space and an
%% arbitrary number of keys to be serviced.

 

The first requirement, that the structural invariants
of the data structure be maintained at the point of transformation demands that
all of the state required to check the structural invariant be present at the
point in the network at which the swap is made. This fact increases the memory
cost on a switch, however with intelligent data structure design the cost of the
required metadata can be mitigated. In the case of Clover, while each key has
an entire linked list history that can potentially span megabytes, the only
required metadata to make the change from CAS to write is the location of the
tail pointer. In this case the metadata cost is O(n) as it grows linearly with
the keyspace.

% \textbf{2) reordering} The second requirement, that operations not be reordered
% after the invariant has been checked requires more care in real systems. For
% instance in an RDMA system with two clients, both could have contesting CAS
% operations swapped with writes. As the two clients are transmitting operations on
% separate QP, and the receiving NIC makes no guarantees about ordering between QP,
% the operations could easily be reordered. In the case with CAS, the order could
% be forced by ensuring that if one write was to succeed the second would fail.
% Without this guarantee the preservation of operation ordering must be maintained
% in another way.

% \subsection{Connection Remapping}


% Our solution here is simple, given that we have the key's for reads and writes
% (Section ~\ref{sec:operation-caching}), all operations for the same key are
% mapped to the same QP.  This algorithm requires that a few pieces of state be
% maintained per connection.  First the sending and response QP for each sender
% and receiver need to be tracked. Second the sequence number of each connection,
% and the original message sequence number offset must be maintained. Per client
% connection the pair of QP's require 48 bits, and the sequence + message sequence
% require an additional 48 for a total of 12 bytes per connection. The storage
% requirement for mapped requests varies based on the algorithm. If clients are
% able to issue an unbounded number of async requests, then a buffer large enough
% to maintain backwards mappings for each request is required. In clover clients
% can issue up to 2 async requests, so we keep a two 6 byte mappings for each
% connection available to map back. 

% Depending on the algorithm and the QP mapping scheme requests from a single
% sender can be reordered. That is, if a client makes a read and write request to
% different locations in memory, and they are mapped to different QP, they may be
% returned out of order. Infiniband allows for out of order operations on
% receivers~\cite{infiniband-spec}, which pushes operation ordering to client
% side user space. Roce does not allow for out of order operations. In this case
% the receiving NIC will retransmit if requests are delivered out of order. Here
% we buffer requests in network, as we have application knowledge the size of the
% buffer is bounded (to the size of a single read packet in clovers case). We
% suggest that given the tight memory restrictions on middleboxes algorithms which
% have an unbounded number of async requests leave the ordering of remapped
% requests to client side user space using IB verbs or a different transport layer
% entirely.

\todo{these sections may not be nessisary}

\subsection{RDMA interposition}

RDMA packets are not intended to be modified in
flight, and care must be taken not to corrupt them. RDMA invariant CRCs (ICRC)
are calculated at the time of sending and are designed to ensure the integrity
of the payload. When we modify \texttt{c\&s} packets their ICRC must be
recalculated or the packet will be rejected by the receiving NIC. FPGA
implementations of RDMA ICRC have been built in the past~\cite{Mansour_2019};
the required CRC calculation is identical to Ethernet CRC, with some additional
header components and field masking.  Our DPDK solution uses \texttt{zlib}'s
\texttt{crc32} for the calculation.  We believe that this algorithm can also be
implemented efficiently on a programmable switch.

