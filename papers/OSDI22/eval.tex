\section{Evaluation}
\label{s:results}

Adding computation to the network increases latency and consumes memory. Our
evaluation demonstrates that by applying our techniques we are able to see
significant performance boots, while consuming only a small amount of memory.

\subsection{Testbed}

% ACS - Lifted from WORDS; rewrite

Our testbed consists of five machines: a Clover memory server,
metadata server, and two Clover clients; the last machine hosts
{\sword}. Physically, the machines are identical: each is equipped
with two Intel Xeon E5-2640 CPUs and 256 GB of main memory evenly
spread across the NUMA domains. Each server is equipped with a
Mellanox ConnectX-5 100-Gbps NIC installed in a 16x PCIe slot, all of
which are connected to a 100-Gbps Mellanox Onyx Switch. All Clover
servers are configured with default routing settings: clients send
directly to the metadata and data servers. We install OpenFlow rules
on the Onyx switch to redirect the Clover RDMA traffic to \sword; \textbf{XXX:} Figure~\ref{f:testbed} shows the layout of our
testbed.

\subsection{YCSB Benchmarks}

The ycsb benchmark consists of varying read and write workloads which have been
shown to emulate many common data center operations. We show a breakdown of our
techniques, mainly read and write caching, QP mapping, and atomic replacement
with respect to their effect to system performance on two YCSB benchmarks. We
choose YCSB-B (95\% read and 5\% write) as our baseline, and YCSB-A (50\% read and
50\% write) to demonstrate how our algorithm performs under high contention. We
also show the performance boots obtained while running a 100\% write workload
which is intended to emulate other programmatic workloads such as accessing a
lock in remote memory.

\begin{figure*}
    \includegraphics[width=1.0\textwidth]{fig/full_system_performance.pdf}
    \caption{{Performance increase of each technique using clover as a baseline on YCSB benchmarks.}}
    \label{fig:full_system_performance}
\end{figure*}

\todo{real takeaways}

\subsection{Memory Utilization}

Our techniques give a performance boots at the cost of in network memory. We
took special care to design our algorithms so that they could 1) use only a
small amount of network memory, 2) be scalable depending on the resources
available. We show how our performance varies as a function of the available in
network state.

As seen in Figure~\ref{fig:cache} our write caching is able to provide a
significant performance boost while only using a small number of cached
addresses. In the following experiment we show the maximum performance boost we
can provide as a function of the available in network memory. Specifically in
the case of read and write caching this means shrinking the size of the
available cache. In terms of QP mapping it restricts the number of connections
which can have their connections mapped. Unmapped connections must use atomic
operations for their requests to succeed.

\begin{figure}
    \includegraphics[width=0.45\textwidth]{fig/memory_util.jpg}
    \caption{{Relative performance improvement of our techniques with restricted amounts of memory. Here a rightsized allocation implies that for the given number of connections we could support, all requests were mapped and reads and writes were cached.}}
    \label{fig:memory_util}
\end{figure}

\todo{say something real about the the memory utilization takeaways}


\subsection{Bandwidth Reduction}

Placing memory operations in band with regular network traffic can be
problematic as applications remote memory usage has the potential to vary
dramatically per application. When under contention resources require additional
packets which inflate the bandwidth necessary for a single operation. Our in
network steering algorithm, removes the need for operations to retry.
Figure~\ref{fig:bandwidth_reduction} shows the percentage of bandwidth reduced
per operation when resources are contested under different workloads.

\begin{figure}
    \includegraphics[width=0.5\textwidth]{fig/bandwidth_reduction.pdf}
    \caption{{Bandwidth reduction when in network steering is applied}}
    \label{fig:bandwidth_reduction}
\end{figure}

\todo{real takeaways}

\subsection{Tail Latency}

\begin{figure*}
    \includegraphics[width=1.0\textwidth]{fig/99th_latency.pdf}
    \caption{Left: Read tail latencies 99th percentile. Right: Write latencies. Each measure taken on a zipf distribution of requests with 64 clients.}
    \label{fig:tail_latency}
\end{figure*}

\subsection{CAS to write performance}

\begin{figure}
    \includegraphics[width=0.5\textwidth]{fig/cas_vs_swap.pdf}
    \caption{Performance Improvement from Swapping CAS to Write on a single key}
    \label{fig:cas_vs_swap}
\end{figure}

We show the raw performance improvement of swapping compare and swap operations
to writes by running an experiment which explicitly tests CAS contention. Each
client core is given a single QP, and all clients make CAS requests to the same
shared location. We measure the performance as the number of clients requesting
the same shared address is increased. All requests are routed through our
middlebox. In the default case, cas operations flow through without
interference. We set the number of cores on our middlebox to 24 so that in our
maximal test case each client thread flows through exactly one middlebox core
for the lowest degree of interference between QP. In our CAS to Write
configuration all client cores request the same address, and as such all are
routed onto the same destination QP. Note that this configuration has the
highest degree of contention for our middlebox as 24 client threads must be
multiplexed to and from a single client connection. Further due to DPDK queue to
core partitioning requirements all TX for a destination QP must be done by a
specific core. Because of this requirement all requests must flow through a
single core prior to being issued to the memory side NIC.

Figure~\ref{fig:cas_vs_swap} Show the performance improvement gained by
converting CAS operations to writes. Both configurations hit distinct
bottlenecks. CAS operations bottleneck due to being applied to a single key. In
the case of converting CAS to write, the bottleneck is the processing power of
our CPU cores. The maximum throughput our prototype can process is 2.8 million
operations per second per core. In this configuration all requests to the memory
server must be processed by the same TX core. As such our bottleneck is
approximately 2.8 MOPS. Hardware implemented CRC, cache tuning, and better lock
management for TX queues could yield higher per core performance in the future.



