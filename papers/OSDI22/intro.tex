\section{Introduction}

Resource disaggregation, and specifically memory disaggregation incurs serious
performance penalties due resources being exclusively accessed over a network.
The latency of making a memory access over the network is significantly higher,
on the order of 20x, than a local access (50ns local and ~1us remote).  While
this cost may seem prohibitive the benefits of resource disaggregation, mainly
high memory utilization, arbitrarily flexible resource allocation, and energy
efficiency~\cite{zombieland} make the architectural paradigm tantalizing. In
response a variety of Protocols for accessing remote memory have been developed
and productionized: RDMA~\cite{infiniband-spec}, CXL~\cite{cxl},
GenZ~\cite{genz} and Omni-Path~\cite{omni-path}. Each protocol, while distinct,
meets approximately the same requirements, reliable access to byte addressable
remote memory with low latency and high throughput.

%% it would be nice to Cite SUPERNIC and CLIO here but I'm not sure it makes
%sense untill it's published at a major venue
%%Clio~\cite{clio-arxiv}
%%todo ask alex about the archive reference

%%todo do a quick read of how dma is delt with on the other interconnects
Regardless of the memory transport technology the overarching scheme is the
same, clients request remote addresses which is translated and transported via  
DMA capable network devices such as commodity RDMA capable NICs~\cite{connectx},
SOC~\cite{cavium} or FPGA smartNICs~\cite{corundum,kv-direct} or
DPUs~\cite{fungible}.  These devices have the ability to execute operations,
such as read, write and compare and swap directly on host memory across PCIe,
however for the convince of serialization they typically coordinate with the CPU
on the socket at which the DMA is being performed. This is the norm for high
performance RDMA key value stores~\cite{cliquemap,erpc,herd,sonuma,storm}. 

In the absence of a CPU (the disaggregated case), it's up to the client to check
that its reads and writes are serialized. Doing so typically requires the use of
expensive locking operations such as compare and
swap~\cite{design-guidelines,clover}.  The penalty for client side serialization
is stark, in prior disaggregated prototypes clients simply partition memory
completely and forgo any sharing ~\cite{reigons,fastswap, legoos}. The few
disaggregated memory systems which provide sharing have made use of state of the
art data structures to amortize the cost of contention. For example:
Clover~\cite{clover} (a disaggregated key value store) a great deal of care is
taken to avoid both memory conflicts and the need to perform multiple reads or
writes to complete an operation. To achieve this Clover caches metadata about
the location of the latest writes and reads while also make use of a remote data
structures which allows for lockless reads. In the case of highly contended
resources however the performance of clover diminishes sharply due to an
increased number of atomic locking operations required on writes.

In this work we make the observation that given a rack-scale disaggregated
system memory operations are serialized by a centralized interconnect (either a
Switch or some other middlebox~\cite{disandapp}). This
physical layer serialization imposes a globally observable total order on memory
operations within a rack. As such this centralized location can be used to
resolve contention to shared resources, provide locking, and remove the need for
expensive atomic operations on an RDMA enabled NIC. To show the benefits of such
a system we implement a variety of in network contention resolutions for the
Clover protocol and demonstrate their performance boots. Specially we cache the
locations of the most recent reads and writes with stale metadata are steered to
the most up to date locations allowing for O(1) remote memory accesses in all
cases.

We demonstrate in network serialization removes the need for expensive locking
operations on the NIC. Using RDMA transport information and clover specific
application knowledge all reads and writes to contended areas are totally
ordered in the network. Specifically all reads and writes to the same keys are
multiplexed to the same queue pairs, by utilizing the RDMA ordering requirements
of QP's reads and writes require no expensive locks and can flow at line rate to
remote memory. This ordering requires a number in band adjustments to the RDMA
protocol in order to interoperable with commodity hardware. QP state must be
maintained in network, specifically the sequence numbers of multiplexed
requests, so that response packets can be demultiplexes back to their original
connections. Small adjustments such as generating acks for collapsed requests is
also required. We demonstrate that these algorithms are implementable in network
at little cost with a DPDK prototype. We measure that ~\todo{we achieve a ?X
improvement in performance using only XMB of in network state, and ?X
performance improvement in highly contested settings with full use of system
memory}.
