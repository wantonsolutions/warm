\section{Background}

The literature on disaggregated memory, efficient use of RDMA verbs,
and programmable middleboxes are each too vast to detail here.
Rather, we briefly highlight relevant core themes from each, and then
provide a short overview on concurrency management in Clover to
facilitate understanding of \sword's optimizations.

\subsection{Resource disaggregation}

Within the disaggregation community there is a divide between software
and hardware-based approaches to remote memory management.  The use of
dedicated hardware to access and control memory over the network is
often referred to as memory disaggregation, in which custom hardware
is used to interpose on either the caching or paging
system~\cite{dredbox,rethinking}.  Alternatively, pproaches that rely
entirely on commodity hardware to manage remote memory are generally
referred to as far memory~\cite{reigons,fastswap, legoos,
  clover, lite}.  We concentrate on the latter as they are far more
likely to see deployment in the short term; our techniques presume
only standards-compliant RDMA NICs and are designed to leverage the
scalability of today's programmable switching hardware.

Choosing how to expose remote memory to application remains an open
problem. Full transparency implies that existing applications can make
use of remote memory without modification. Many systems leverage
virtual memory and use remote memory as a swap device fetching and
evicting pages to and from remote
memory~\cite{fastswap,GMS,infiniswap,leap,ramcloud}. Transparency
comes at a cost, however, and systems that make remote accesses
explicit typically have far higher performance for similar
operations~\cite{aifm}.  Most existing transparent remote memory
systems do not support sharing, as unmanaged contention for resources
such as shared locks can have disastrous performance implications.  We
hope that {\sword} may enable progress in this space by dramatically
decreasing the frequency of data races.

%We
%take the latter approach and suggest that to achieve the highest
%performance programs for remote memory should be built by engineers
%who understand the constraints of remote memory, and expose common
%API's to users, such as POSIX, PUT/GET, or language integrated
%runtimes.

\subsection{RDMA key-value stores}

Given the challenges inherent in full-blown remote memory, many have
focused on more constrained, but broadly applicable storage
abstractions like key-value storage.  RDMA has been used in a litany
of work to build fast in-memory key-value
stores~\cite{farm,MemC3,herd,pilaf,sonuma,storm}. In their quest for
high performance, these works provide deep insights into the
trade-offs between different RDMA verbs and the spectrum of
remote-memory data structures. In each, however, it is assumed that a
remote CPU is co-resident with memory to provide some degree of
serialization for writes and metadata manipulation. Yet, in the context of
truly passive far memory no such co-resident CPU exists.  In
its absence clients must enforce serialization for potentially
conflicting operations through RDMA atomic requests. These special
hardware-supported requests are expensive and known to (dramatically)
underperform fully asynchronous verbs, both in terms of throughput and
scalability~\cite{design-guidelines}.

\subsection{Programmable middleboxes}

Most current proposals for fully disaggregated compute architectures
are scoped to rack
scale~\cite{disandapp,the-machine,intel-rack,firebox}.  The machinery
for arbitrating memory access varies between proposals but usually
relies on a centralized controller.  Some are as simple as a
scaled-out PCIe root complex, while others imagine a programmable
middlebox with an API exposed to applications~\cite{disandapp}. We
see the latter as a promising opportunity as it allows developers to
highly optimize their programs for remote memory.  In this work we
envision rack-scale computing with either a programmable switch, or
some other programmable hardware (e.g., FPGAs) being used as a
centralized switching fabric for memory operations~\cite{supernic}. We
assume that, like programmable switches, these middleboxes are highly
resource-constrained with a small amount of SRAM intended for
forwarding packets and limited functionality for executing programs in
the data path.

\subsection{Clover}

In order to demonstrate the effectiveness of {\sword} in practice, we
needed to select a passive far memory system with which to experiment.
The authors of Clover, a recently published key-value store designed
for disaggregated persistent memory~\cite{clover}, made their
implementation publicly available and helped us deploy it on our
testbed. While Clover targets persistent storage it is also a
prototypical example of a key-value store for remote DRAM.  Most
importantly for our purposes, its design makes the assumption that
there are no remote CPU's co-resident with memory. All of Clover's
remote memory accesses are made via one-sided RDMA requests: reads,
writes, and compare and swaps.  In order to improve performance,
Clover moves metadata storage off of the data path. On the data path,
reads and writes for a given key are made exclusively to an
append-only linked list stored in remote memory. All RDMA requests are
made to the (presumed) tail of the list. A client may not know the
location of the tail as other writers may concurrently push it
forward.  When a read or a write fails to land at the tail of the list
Clover iteratively traverses the structure until the tail is
found. While this provides no liveness guarantees, in the common
read-heavy case concurrent clients all eventually reach the end of the
list. To speed up operations clients keep caches (i.e., hints) of the
end of each key's linked list to avoid traversals. When writes are
heavy and/or when particular keys are hot, Clover's performance
degrades substantially. Clover serializes writes with RDMA compare and swap
requests which further impact performance.

%which
%when used frequently on the same location lead to abysmal
%performance. In cases when CAS's fail, clover performs a read for the
%tail and then retries the CAS. Therefore this number inflates as the
%number of clients and conflicts increase without guaranteeing fair
%forward progress.
