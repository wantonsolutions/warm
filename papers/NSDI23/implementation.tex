\section{Implementation}

\begin{figure}
  \center
    \includegraphics[width=0.45\textwidth]{fig/packet_processing.pdf}
    \caption{\sword's packet processing pipeline}
    \label{fig:system}
  \vskip -0.5em
\end{figure}

We implement {\sword} in P4 and DPDK. Our P4 implementation includes the logic
for identifying and tracking connections, and resolving read and write conflicts
in Clover. Connection multiplexing is implemented separately on a DPDK
middlebox discussed further in this section.

\subsection{P4 implementation}

Read and write steering are straightforward to implement in P4. P4
\textit{registers} maintain state in SRAM. We use registers to store connection
state, virtual addresses, and outstanding requests. Registers in P4 are highly
constrained, the number of registers is vendor specific and their width 32, 16,
8 bits are predefined. P4 switches execute in stages, 16 ingress and 16 egress
on our switch. Register lookups are done once per packet, and are pinned to a
stage.  Therefore our p4 logic must be pipelined correctly so that write
requests store their address in thee same stage that CAS and read requests look
them up. One constraint with fixed with registers is that large lookups must be
done across multiple stages. We use two stages to lookup virtual addresses (64
bits) with two 32 bit registers, and single stage lookups for queue pairs,
sequence numbers, and connection id's. {\sword} switch resource utilization is
detailed in Appendix~\ref{sec:appendix_resources}.
%%
Parsing and traffic identification are simple in P4. We use the same header
parser at the P4 simple switch to parse up to the UDP header, and created our
own header parser for RoCEv2 and Clover headers.  {\sword} uses RoCEv2 header
information, and clover headers to identify traffic for steering. When Clover
traffic is identified the connection is added to our connection tracker, and a
connection id is generated. RDMA invariant CRC's are tricky as P4 hardware does
not support generating them directly. We bypass them in P4 by turning off the
ICRC check on our CX5 NICs, the ethernet CRC provides the same integrity check
at rack scale. In DPDK we implement the ICRC
recalculation(Appendix~\ref{sec:appendix_icrc}).

% P4 is a highly constrained language. For example it does not support loops.
% Architecturally P4 switches are also highly constrained. Packet parsing must be
% performed using the limited TCAM resources available on the switch, conditionals
% have a limited branching factor (4). Packets are processed in stages, and device
% memory from one stage cannot be accessed at the next, therefore branching
% conditionals which access the same variables must be aligned to the same switch
% stage if they are to read or write to the same variables. In this section we
% describe our P4 implementation of {\sword} and how it adheres to the constraints
% of P4 and programmable switches.

% \textbf{Parsing} {\sword} has no special requirements for packet parsing. Our
% design is focused on RoCEv2 which runs over UDP. Our prototype implements
% Ethernet, IP, UDP and RoCEV2 parsing. RoCEv2 headers are required for reading,
% and manipulating virtual addresses. One additional header is required for Clover
% to read the key out of write requests.

% \textbf{Resource Utilization} {\sword} requires SRAM to cache data structure
% information. The amount of data required is dependent on the data structure
% itself. In clover we cache the last virtual address of each key, and we keep a
% cache 3x the size of the keyspace for read steering.


% \textbf{Traffic identification} Depending on the disaggregated
% rack architecture, memory traffic might be coresident with regular
% network traffic.  Additionally some of the traffic on the memory bus
% may not require tracking or manipulation. In the case of Clover we do
% not interpose upon or modify traffic to the metadata server as it is
% not in the read/write path. The first stage of our packet-processing
% pipeline classifies requests for manipulation. In our design operators
% submit a filter as part of their configuration to allow traffic which does
% not need to be modified to flow freely.

% \textbf{Dynamic connection tracking}
%  A key goal of our approach is to support
% serializer-based performance enhancements without modifying the far memory
% system itself---including requiring any explicit negotiation with the
% serializer.  We add and subtract RoCE connections to {\sword}'s management
% tables based upon the send and receipt of CAS operations. The QP and sequence
% number for the CAS are stored on send, and the ATOMIC ACK is used to obtain the
% other recever's QP.  As this approach requires only a single packet, requests
% can be added and removed from our algorithm dynamically with little effort.

%For instance is designed to deal with memory operations made to the wrong
%location via iterative pointer chasing. We strongly suggest that disaggregated
%algorithms take this approach as our middlebox solution only acts to acclerate
%operations in the common case.


% \textbf{Initializing connection mapping}
% \sg{dpdk only}
% As some state may be dependent on the number of connections (such as the
% key-to-QP and lock-to-QP mappings), state transitions either require a lock, or
% the copying of current state over to a new epoch when new connections are added.
% In all of our experiments only one such transition is made. We begin our mapping
% after a specific number of clients for the experiment have connected. Once the
% total number of clients have connected, a switch is flipped, and the QP
% multiplexing algorithm begins. Requests which do not have mappings stored, but
% were in flight during the flip have their sequence numbers and MSN values
% applied to the connection state of the new epoch.

%\subsection{Operation caching}
%\label{sec:operation-caching}



% \begin{figure}
%     \includegraphics[width=0.45\textwidth]{fig/throughput.pdf}
%     \caption{Default Clover throughput vs. Clover with write conflict
%     detection and correction turned on \todo{recompute with the read caching values (old)}}
%     \label{fig:throughput}
%     \vskip -1em
% \end{figure}

% \subsection{Implementing Atomic replacement}

% In the
% following subsections we describe the dangers of removing atomics, and present
% our solutions.

% A few assumptions must be made in order for this replacement of operations to be
% made. First and foremost all operation serialization must be made, and finalized
% at the point where the CAS is swapped out. More formally, all of the data
% structure invariants which required locking, must be satisfied at the time of
% transforming the packet. Further the order of operations must be maintained
% downstream from the checking of the invariant. These two requirements influence
% the design of any system which aims to make this performance improvement.


%% ACS - This is just lifted from WORDS; no need to repeat here

%% \begin{figure}
%%     \includegraphics[width=0.45\textwidth]{fig/cache.pdf}
%%     \caption{Performance as a function of keys cached. Caching a few
%%     of the top-$N$ keys provides the greatest marginal throughput
%%     benefits.}
%%     \label{fig:cache}
%% \end{figure}

%% \textbf{reduced cache size} we show that if hot keys are known we require only a
%% small amount of in network state~\ref{fig:cache} we have considered dynamic
%% approaches such as LRU which would allows for a finite amount of space and an
%% arbitrary number of keys to be serviced.

% The first requirement, that the structural invariants
% of the data structure be maintained at the point of transformation demands that
% all of the state required to check the structural invariant be present at the
% point in the network at which the swap is made. This fact increases the memory
% cost on a switch, however with intelligent data structure design the cost of the
% required metadata can be mitigated. In the case of Clover, while each key has
% an entire linked list history that can potentially span megabytes, the only
% required metadata to make the change from CAS to write is the location of the
% tail pointer. In this case the metadata cost is O(n) as it grows linearly with
% the keyspace.

% \textbf{2) reordering} The second requirement, that operations not be reordered
% after the invariant has been checked requires more care in real systems. For
% instance in an RDMA system with two clients, both could have contesting CAS
% operations swapped with writes. As the two clients are transmitting operations on
% separate QP, and the receiving NIC makes no guarantees about ordering between QP,
% the operations could easily be reordered. In the case with CAS, the order could
% be forced by ensuring that if one write was to succeed the second would fail.
% Without this guarantee the preservation of operation ordering must be maintained
% in another way.

% \subsection{Connection Remapping}


% Our solution here is simple, given that we have the key's for reads and writes
% (Section ~\ref{sec:operation-caching}), all operations for the same key are
% mapped to the same QP.  This algorithm requires that a few pieces of state be
% maintained per connection.  First the sending and response QP for each sender
% and receiver need to be tracked. Second the sequence number of each connection,
% and the original message sequence number offset must be maintained. Per client
% connection the pair of QP's require 48 bits, and the sequence + message sequence
% require an additional 48 for a total of 12 bytes per connection. The storage
% requirement for mapped requests varies based on the algorithm. If clients are
% able to issue an unbounded number of async requests, then a buffer large enough
% to maintain backwards mappings for each request is required. In clover clients
% can issue up to 2 async requests, so we keep a two 6 byte mappings for each
% connection available to map back. 

% Depending on the algorithm and the QP mapping scheme requests from a single
% sender can be reordered. That is, if a client makes a read and write request to
% different locations in memory, and they are mapped to different QP, they may be
% returned out of order. Infiniband allows for out of order operations on
% receivers~\cite{infiniband-spec}, which pushes operation ordering to client
% side user space. Roce does not allow for out of order operations. In this case
% the receiving NIC will retransmit if requests are delivered out of order. Here
% we buffer requests in network, as we have application knowledge the size of the
% buffer is bounded (to the size of a single read packet in clovers case). We
% suggest that given the tight memory restrictions on middleboxes algorithms which
% have an unbounded number of async requests leave the ordering of remapped
% requests to client side user space using IB verbs or a different transport layer
% entirely.


% \subsection{DPDK considerations}

% {\sword} is implemented in DPDK for ease of programmability, but
% requires that all requests are processed without the aid of RDMA
% hardware on the NIC. Using DPDK introduces issues in terms of
% performance as individual cores on the server have low packet
% processing capabilities relative to dedicated networking hardware. As
% such our design is multithreaded and requires the use of careful
% atomics for performance.  Our design partitions cores into TX and RX
% groups. RX cores are configured via RSS and perform traffic
% identification, write steering and mapping. We completely removed the
% need for any explicit locking between cores and share as little state
% as is theoretically possible. Each TX core serializes requests by
% making atomic fetch-and-add updates to a shared region of memory
% containing connection sequence and message-sequence numbers. RX cores
% are handed packets from TX cores using DPDKs lock-less ring
% library. TX cores check packets to determine if they require CRC
% computation and calculate the CRC only if the packet has been
% modified. Adding a packet handoff between TX and RX increases latency,
% however the head-of-line blocking incurred by having a single core
% perform both mapping and CRC calculations is too high for our purposes.

% We find that by using an array size of
% 3$\times$ the vast majority of reads succeed first try.
% For
% performance reasons we forgo heavyweight hash functions and use
% the \texttt{murmur3} bit scrambler to attain an approximately even
% hash of virtual addresses in only a few cycles.

\subsection{DPDK vs P4}

{\sword} is implemented in P4 and run on a programmable switch, however its
connection mapping functionality is only implemented in DPDK due to hardware
limitations on the programmable switch, and on ConnextX-5 NICs. In this section
we describe why the space and logical complexity of tracking outstanding
requests, and reversing the effects of ACK coalescing is prohibitively hard on
current hardware and suggest small hardware modifications which would make the
implementation simple.

\textbf{Connection Mapping:} Connection mapping requires the storage of a 64 bit
mapping entry for each in flight request (old\_seq, new\_seq, conn\_id,
cas\_to\_write, opcode). These map entries are space concern on programmable
switches. Mapped entries are stored per connection. When an request is mapped to
a connection the entry is generated, when a response returns the entry is
removed.In the worst case, any connection may need to store outstanding requests
for the total number of connected clients as each client could asynchronously
issue a request for the same lock concurrently. 

We use connection sequence numbers to look up requests as they are unique for
each connection, have no collisions, and only require an offset for lookup
($sequence\_number \% max\_clients$). The programmable switch requires that all
storage be allocated at compile time, so we need to provision for the worst case
scenario.  The space requirement grows $O(n^2)$ with the number of clients as
each connection must be statically allocated with enough space to handel a worst
case burst.

Given static allocation if we want to support 512 client threads (the order of
our experiments), in the worst case we need 1024 * 1024 * 8 bytes of connection
storage. 8MB is not much storage, but on our Tofino switch this represents 1/4
of the total SRAM assuming the compiler packs requests perfectly. Unfortunately
this is unlikely the case.  Making this mater worse is that much of switch
memory is allocated into 8 and 16 bit registers. Only a few are allocated to
read and write 32 bits. 2 pipeline stages are the minimum for connection
remapping storage. In the worst case (using 8 bit registers) 8 are required (an
entire pipeline). While expensive this is not a prohibitive cost to
implementation.

We could use a hash table shared by all connections as a lookup for mapping
entries using the key $concat(qp\_id,sequence\_number)$ which would conceptually
reduce the space complexity. However if a collision occurs the collision lookup
must be done in a following stage of the pipeline. This increases the space cost
by the size of the longest collision chain. It also increases the size of each
entry from 64 to 88 as the original queue pair id must be stored alongside the
map entry to detect collisions. This requiring 3 stages to store. Detecting a
collision requires an additional stage to run a conditional statement on the
queue pair id. A chain of 2 collisions therefore would require 8 stages -- our
entire egress pipeline to resolve. Given that we are processing millions of
packets per second on a maximum sized hash table of 16k entries, collisions of 3
or more are common.

\textbf{RDMA Coalescing:} Our greatest unmet challenge with connection mapping
in P4 dealing with RDMA coalescing. When a response is mapped back to a client
it may be a coalesced response, if so the prior map entry must be checked and an
ack generated for that entry. This is an iterative process as a single response
can have coalesced many other responses.

In dpdk this search and packet generation implemented in a 3 line function with
a for loop. In p4 it this functionality was prohibitively difficult to engineer.
The complexity is caused by the need to both search multiple entries of the
outstanding connection list which is not supported by P4 table lookups. In P4
each stage of the pipeline holds unique data and supports a single lookup of the
specified register size (8,16,32), as such we are unable to look up a variable
number of requests to generated coalesced ACKs.

One option would have been to duplicate data, so that if a coalesce did occur,
it could be detected by reading duplicate data in the next stage.  The
difficulty is that under aggressive contention the coalescing 10 or more ACKS is
a common occurrence and duplicate stages would be required for each coalesced
response. Given that a 64 bit entry requires two stages duplicating is not an
option as it would easily exhaust all pipeline stages and never guarantee
correctness.

\sg{I don't actually know if the following would work, it's an educated guess:}
Another option is to use packet recirculation. We could duplicate data once, and
perform a look back to the prior entry to determine if it was coalesce and then
generate another packet to handle the handle the coalesced ack. 
%%
Performing this iteratively could generate all the required ACKs. However it
dramatically increases complexity and inverts the order of packet delivery on
the clients.

\textbf{Conditional Complexity:} The final difficulty in implementing connection
mapping in P4 are conditional statements. In P4 architectures branching is
limited. At any conditional a max of 4 paths can exist.  In clover we reduced
the total number of branching statements to 4 for each of the read, write, CAS,
and ACK pathways. Connection remapping requires complex logic and arithmetic for
generating message sequence numbers, storing request stubs across different
register sizes, and detecting, and generating coalesced ACKs.  Separately none
of these issues are enough to prevent the implementation of connection remapping
on a programmable switch. However in concert the P4 complier was not able to
synthesize a program which could fit into our 16 stages of the Tofino 1. Future
hardware such as Tofino 2 and 3 would likely be able to support this
functionality due to having over double the pipeline logic.

\subsubsection{Potential Fixes}

\textbf{Dynamic Memory Allocation:}
Statically allocating 1024 slots for each client is prohibitively expensive as
it grows with $O(n^2)$ due to static memory allocation. The actual required
amount of space is $O(n)$ as that is the total number of in flight requests. If
we could resize our allocations for each connection dynamically (stealing some
memory from under utilized connections) We would be able to decrease our memory
utilization to $O(n)$. This is a trivial task with an allocator and pointers, but
a prohibitively hard task with static registers and single stage lookups.

\textbf{Turn off coalescing} The most complex part of connection remapping is
calculating coalesced ACKs. The fix for this is simple: Turn off ACK coalescing
on the NIC, as it is only a performance optimization. This fix alone would allow
our design to fit onto a Tofino chip.

\textbf{Complex Conditionals} Increasing the branching factor of conditionals
would greatly decrease the complexity of implementing many complex protocol
features. In some cases branching to 4 or more statements would have allowed
lookups in the following stage to be implemented in a single stage rather than
two.

\subsection{Failures}

Clover and Sherman both provide their own degrees of fault tolerance. Clover
never acquires any resources so failures do not result in any stranding, and
Sherman periodically detects if locks were kept by a dead client. At an
operational level they rely on RDMA to provide in order delivery of their
messages and react to failed CAS operations by retrying their operations.

{\sword} must therefore be able to detect packet retransmissions. If a packet is
dropped in transit sword must not treat the repeated packet as a new operation.
On each RDMA connection we track it's last sequence number, the transformation
{\sword} applied (steering), and if it was ACK'd.  If a retransmission occurs
the same operation is reapplied. This mechanism alone ensures safety in the
majority of packet loss scenarios. However, client failures can cause complex
failure cases.  In practice we consider these failure concerns to be unlikely
during development no packet drops occurred between {\sword} and the memory side
NIC due to clients issuing closed loop operations.


The largest safety concern occurs if a linked-list is broken.  If CAS(A->B) is
issued before CAS(A->C), CAS(A->C) will be write steered to CAS(B->C). If
CAS(A->B) is dropped, CAS(B->C) can still succeed. This creates a broken chain
because the pointer A->B does not exist but B-C does, and {\sword} will mark C
as the tail of the list. In this case the operation CAS(A->B) must be re-issued
to repair the chain.  If the client which issued CAS(A->B) is alive it will
retransmit after a timeout. {\sword} will identify it as a retransmit, and the
packet will be steered to the correct location, reconnecting the chain. However,
if the client side NIC were to fail prior to reissuing the CAS the chain will
remain broken.Here we use an out of band mechanism to repair the chain, on
occasion our control plane queries the switch to check for incomplete CAS
requests, marked as sent but not completed.

\textbf{Connection Remapped Failures:} Unlike in the prior scenario, when
connections are mapped no data corruption can occur. Queue pair operation
ordering ensures that CAS operation on keys are executed in order, and RDMA
go-back-n is triggered if any message is lost.  Retransmissions are more complex
when connection mapping is turned on. If a packet is dropped, and a following
packet is delivered from another client on the same QP, go-back-n will be
triggered by the memory side nic, and all inflight requests on that QP will
become invalidated. When {\sword} sees a go-back-n ack, it triggers the same
mechanism used for ACK coalescing but in reverse. All clients with outstanding
messages are broadcast a go-back-n and asked to retransmit. Note that because no
operations are committed the order and completion of any client operations does
not matter. Clients can fail freely without any concern for data structure
safety.

% Retransmission in this
% case is more complex. A single failed packet will require that all in-flight
% requests from all clients on that QP be retransmitted. If a single message is
% dropped, a following message will trigger a go-back-n response from the memory
% side NIC. We propagate the go-back-n message to all clients with outstanding
% requests using the same mechanism we use to repair coalesced acks, but in
% reverse.  Retransmit requests can be serviced in any order, as none had yet been
% delivered to memory, and if any clients die before retransmitting the structure
% remains safe in spite of their failure.

% One aspect which requires care is maintaining {\sword} state when dealing with
% go-back-n. In the case of Sherman, the value of the lock when a CAS is issued is
% stored in the request mapping stub. If go-back-n is invoked the state of the
% lock is reverted to the value it had just prior to issuing the failed request.
 

% Our goal is ensure that under failure \sword does not introduce any correctness
% issues. Clover ensures end-to-end correctness for packet failures and client
% failures. A key difference between default clover, and \sword enhanced Clover is
% a change in atomic domain. In clover atomic operations are executed by a memory
% side RDMA NIC, in \sword atomic decision are made at the switch. This
% introduces a potential for correctness issues if packets are lost between the
% switch, and memory server, and if clients fail during such events.

% The primary saftey issue introduced by \sword is a broken list. For example if a
% list A->B exists and there are outstanding writes C and D. If concurrent
% requests connect B->C and C->D but the CAS for B->C fails and C->D succeeds this
% creates a broken chain. Subsequent requests will be steered to the disconnected
% chain. One option would be to detect chain breakages and ask clients to reset by
% traversing the chain from root to tail. While tempting this approach could cause
% inconsistancies as clients might read and write data only to be notified of the
% link breakage further down the line. As such we task \sword with enforcing
% atomicity on lost CAS operations between itself and downstream memory servers.

% First we note that RDMA congestion control makes this problem an unlikely
% occurrence on a single hop. During testing we never witnessed packet drop
% between the switch egress port, and RDMA NIC, however corrupted packets, and
% drops could occur.

% If a packet is dropped between \sword and a memory servers NIC, the client will
% time out eventually and retry the operation. We track the last sequence number
% of each client CAS operation, and the virtual address it was steered to. As
% clients are closed loop this is limited to 88 bits per client connection. If we
% see that a client retries a CAS we steer that CAS to the same location it was
% steered to rather than the end of the list.

% In the less likely case that during a dropped packet a client dies, and fails to
% retry the chain could potentially remain disconnected indefinitely. To remedy
% this final case we keep an additional bit set on each client connection marked
% to 1 if a CAS has not been ACKd, and 0 otherwise. Periodically, or upon being
% notified of a client failure, our P4 control plane checks the state of client
% connections and sees if any outstanding CAS operations were never ACKed. If so
% \sword generates the dropped CAS from it's connection table information to
% reconnect the list.

% Note, it is only when CAS packets are dropped from \sword to memory that these
% precautions must be taken. Packets dropped prior to reaching \sword, and on the
% reverse path after operations are committed, and handled by RDMA's go-back-n and
% Clovers recovery mechanisms.



% Steering requests prior to the completion of CAS is safe in most cases while the
% switch is operating correctly and no client failures exist. However in subtle
% cases saftey violations can be introduced if proper measures are not taken. We
% take advantage of the fact that Clover has built in recovery mechanisms. When a
% failure is detected we rely on clover to recover by triggering it's default
% mechanism. By default when clover detects a failure, it traverses the list of
% keys recursively by issuing reads until it finds the new tail.


% \textbf{Cas fails due to not being the tail}
% In the common case B->C will never fail as the switch will steer requests to
% known successful locations. If a successful client request was not routed
% through {\sword} a CAS can return with a failed operation because it was not
% issued to the tail of the list. In this case {\sword} will see the failed
% response and begin by marking the key as invalid. All responses on that key will
% then be changed to failed CAS which will trigger Clovers recovery mechanism.
% \sg{On swordbox we have two options one of which is to examine the failed CAS,
% assuming that it was a correct write that CAS should point to the next valid
% location. Here the switch can simply update its value for the tail of the list
% to the next pointer of the failed CAS}. The second option is to clear the cache
% and then proactively wait for a successful CAS before steering again.

% \textbf{Packet is dropped after Swordbox, and before NIC processing}
% A trickier case in when the CAS operation for B->C is dropped prior to being
% processed by the NIC i.e the packet is dropped due to congestion. Typically this
% will not happen because the NIC generates pause frames before it becomes too
% congested, in our single TOR setup pause frames prevented packet drops due to
% the close loop design of clover. If the drop does occur however it is difficult
% to detect that the chain has been broken. In the common case the client which
% issued the CAS initially will retry. \sg{I did this on the dpdk box} If the same
% request is issued twice {\sword} could attempt to allow the packet to repair the
% broken chain by issuing it again, however this would incur significant buffering
% on the switch to track potentially lost packets. In the case of a retried CAS we
% mark the key a broken and force all clients to retry by marking their next CAS
% as failed.

% \textbf{Packet dropped, client failure}

% If a client's packet is dropped after \sword commits the CAS and the client
% fails before issuing another request, there is no retry to mark the key as
% invalid. In this case we need to refer to a timeout. On each key we keep a 32
% bit register used to mark outstanding requets. Each key also has a counter which
% counts the total number of CAS issued on the key. When a CAS is processed by
% \sword it increments the counter, and then marks a bit in the 32 bit counter
% modulo the packet counter. The number of the request is stored in the connection
% state of the client. When a response comes back on the client connection it the
% bit in the connection counter is set back to 0. When a CAS is issued, if the bit
% in our CAS tracker is set to 1, then a dropped CAS has been identified. In this
% case the key is marked as invalid and clients must attempt to retry the request.
% Out of band the client connections are checked by our control plane, if a rarely
% written key is unacked for a long period of time the key is marked as invalid
% Out of band the client connections are checked by our control plane, if a rarely
% written key is unacked for a long period of time the key is marked as invalid.

% \textbf{Client sends CAS, packet dropped before hitting the nic, and client
% dies/disconnects before retry}

% If a client cas fails, while another succeeds, and {\sword} also fails before
% being able to correct the fault clients the clients could be left in a state
% where some data is detached, and no mechanism which currently exists would
% detect the detachment. Here we suggest that if {\sword} fails clients trigger a
% \textit{read from the start} recovery to ensure that data remains consistent.


% \textbf{Read Steering}

% Reads also represent a tricky case upon write failure. If we use the same write
% tail to steer reads we can end up with inconsistent reads as they may be on a
% detached tail. Assuming that we will not attempt to repair the broken chain by
% inserting a write, the read is invalid.

% We can remedy this by only steering reads to locations that have had their CAS
% acked. This requires maintaining the write ack tail. And will come at the cost
% of performance. under contention.


% \subsubsection{Thoughts}

% \sg{A naive option would be to keep track of uncommited CAS, and then reissue
% them from the switch. This would defer the atomicity to the switch after it was
% effectivly atomicized by the switch. This is the definition of non-soft state
% though.}

% \sg{The key bit is that we can trigger a fault on all future calls to a broken key.}







